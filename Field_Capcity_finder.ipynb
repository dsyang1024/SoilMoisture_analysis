{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e051e6ed",
   "metadata": {},
   "source": [
    "# Field Capacity Finder from the soil moisture\n",
    "\n",
    "## From the soil moisture data\n",
    "***\n",
    "### Data description\n",
    "- data is being collected from the F70 field.\n",
    "- soil moisture is in unit of %, **not cleaned**\n",
    "- this script includes cleaning function\n",
    "- this script designed to be used in daily basis.\n",
    "- from the soil moisture monitoring website\n",
    "- example data is 'Calc_def_test.csv'\n",
    "\n",
    "- [soil moisture data](https://things.iot.ag.purdue.edu:8080/dashboard/dc56c5a0-ee3e-11ec-b72b-5dd76ca52a2b?publicId=a914a590-ecae-11ec-b72b-5dd76ca52a2b) input data for the script will be downloaded directly\n",
    "> - soil moisture data is from Purdue AgIT server that collects the field data through LoRaWan network\n",
    "> - the data should be downloaded from the 'Summary Data Table' tab\n",
    "> \n",
    "- [weather data] input data for the script will be downloaded directly\n",
    "> - weather data is from Purdue Mesonet server and using ACRE station data\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac09df",
   "metadata": {},
   "source": [
    "For downloading the data\n",
    "  - Summary data table\n",
    "  - from date1 to date2\n",
    "  - in csv form\n",
    "  - file name does not matter or add today's date at the end of the file name.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0bedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:  https://things.iot.ag.purdue.edu:8080\n",
      "Token: eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ5YW5nMjMwOUBwdXJkdWUuZWR1IiwidXNlcklkIjoiNjRlOWZjYjAtZjc0ZS0xMWVlLWIzYmMtN2ZlNjliZjhkNDExIiwic2NvcGVzIjpbIkNVU1RPTUVSX1VTRVIiXSwic2Vzc2lvbklkIjoiM2I5NjY2YTQtOGM2OC00YjliLWEwYWMtNzY5MDhkNWNkNWRjIiwiaXNzIjoidGhpbmdzYm9hcmQuaW8iLCJpYXQiOjE3MTI4NDY1MjIsImV4cCI6MTcxMjg1NTUyMiwiZmlyc3ROYW1lIjoiRG9uZ3Nlb2siLCJsYXN0TmFtZSI6IllhbmciLCJlbmFibGVkIjp0cnVlLCJpc1B1YmxpYyI6ZmFsc2UsInRlbmFudElkIjoiYWFjNjU1YTAtYWM2Mi0xMWVjLWFiYzgtMWYxYzA5NTgwZTY3IiwiY3VzdG9tZXJJZCI6Ijc1NzZiMDIwLWVjYWUtMTFlYy1iNzJiLTVkZDc2Y2E1MmEyYiJ9.N0PmsYDLhArorybDQkqWSBevXFMWZRngqi9C4pA48sav5Gs9nvr9cpiBZhviAIJFaEzVyRbNaPNLyXpdwwrRpg\n",
      "2024-02-11 18:00:00 2024-04-11 06:00:00\n",
      "1707674400000 1712815200000\n",
      "[['137feb20-0ea0-11ed-ba72-9f81aa692cc5',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-2491',\n",
      "  'a84041315185231b'],\n",
      " ['1b7224a0-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0113',\n",
      "  'a840415d3184c7c9'],\n",
      " ['3db697b0-0934-11ee-a944-3b605f1e54fb',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0554',\n",
      "  'a8404185a187c722'],\n",
      " ['471295f0-0ea0-11ed-ba72-9f81aa692cc5',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-2492',\n",
      "  'a8404116d185231c'],\n",
      " ['51591fb0-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0115',\n",
      "  'a84041359184c7cb'],\n",
      " ['62398430-0934-11ee-a944-3b605f1e54fb',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0555',\n",
      "  'a8404106a187c723'],\n",
      " ['636750f0-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0118',\n",
      "  'a84041735184c7ce'],\n",
      " ['702b4f60-0934-11ee-a944-3b605f1e54fb',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0556',\n",
      "  'a84041506187c724'],\n",
      " ['7fef4280-0934-11ee-a944-3b605f1e54fb',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0557',\n",
      "  'a8404167b187c725'],\n",
      " ['89046000-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0112',\n",
      "  'a840415b7184c7c8'],\n",
      " ['8a3b8550-0934-11ee-a944-3b605f1e54fb',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0558',\n",
      "  'a84041fcc187c726'],\n",
      " ['9d46a8a0-3530-11ee-b689-09c03edf71a6',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0629',\n",
      "  'a840416531882915'],\n",
      " ['a0c13010-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0116',\n",
      "  'a84041b59184c7cc'],\n",
      " ['b2002a70-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0117',\n",
      "  'a84041f11184c7cd'],\n",
      " ['c55b4e10-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0120',\n",
      "  'a84041715184c7d0'],\n",
      " ['c62c04e0-3530-11ee-b689-09c03edf71a6',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0630',\n",
      "  'a84041b3f1882916'],\n",
      " ['d5639d30-ecaf-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0114',\n",
      "  'a84041124184c7ca'],\n",
      " ['ec32ce60-ecae-11ec-b72b-5dd76ca52a2b',\n",
      "  'ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-0111',\n",
      "  'a840419db184c7c7']]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# // this part is download the data from the website for 2 month.\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pytz\n",
    "import config\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "deviceList = []\n",
    "\n",
    "# ** set the configuration for the request                                                                                  **\n",
    "config = {\n",
    " 'username' : 'yang2309@purdue.edu', ### Insert your email address used by AgIT Thingsboard system\n",
    " 'password': 'dsya2002',  ### Insert your AgIT thingsboard password\n",
    " 'server' : 'https://things.iot.ag.purdue.edu:8080'\n",
    "}\n",
    "\n",
    "# ** defining the function to get the token for the request and setting the header for the request                          **\n",
    "def getCustomerDevices(custID, textSearch=None):\n",
    "    parameters = {        \n",
    "        'pageSize': 1000,\n",
    "        'page': 0,                \n",
    "    }\n",
    "    att_parms = {\n",
    "        'keys': 'dev_eui'\n",
    "    }\n",
    "    if(textSearch):\n",
    "        parameters.update({'textSearch': textSearch})\n",
    "    responseList = requests.get(f\"{config['server']}/api/customer/{custID}/devices\", headers=TBheaders,params= parameters).json()\n",
    "    #pprint(responseList)\n",
    "    list = []\n",
    "    for dev in responseList['data']:\n",
    "        #pprint(dev)\n",
    "        #print('------------------------------------------------------------------------------------------')\n",
    "        #'id': {'entityType': 'DEVICE', 'id': 'd49153a0-c868-11eb-95d8-09d06ef6a9a5'},\n",
    "        url = f\"{config['server']}/api/plugins/telemetry/DEVICE/{dev['id']['id']}/values/attributes\"\n",
    "        deviceResp = requests.get(url, headers=TBheaders,params= att_parms).json()\n",
    "        #print('------------------------------------------------------------------------------------------')\n",
    "        list.append([dev['id']['id'],dev['name'],deviceResp[0]['value']])\n",
    "    return list\n",
    "        \n",
    "\n",
    "def login(url, username, password):\n",
    "    # Log into ThingsBoard\n",
    "    return requests.post(f\"{url}/api/auth/login\", json={\n",
    "        \"username\": username,\n",
    "        \"password\": password\n",
    "    }).json()['token']\n",
    "\n",
    "def get_keys(device):\n",
    "    return requests.get(f\"{config['server']}/api/plugins/telemetry/DEVICE/{device}/keys/timeseries\",\n",
    "                 headers=TBheaders).json()\n",
    "def get_data_chunk(url, token, device, key, start, stop, limit):\n",
    "    #print([url, device, key, start, stop, limit])\n",
    "    return requests.get(f\"{url}/api/plugins/telemetry/DEVICE/{device}/values/timeseries\",\n",
    "             headers=TBheaders,\n",
    "            params= {\n",
    "                'keys': key,\n",
    "                'startTs': start,\n",
    "                'endTs': stop,\n",
    "                'limit': limit,\n",
    "                'agg': 'NONE'\n",
    "            }).json()\n",
    "\n",
    "def get_data(url, token, device, key, start, stop):\n",
    "    global totalLength\n",
    "    p = pd.DataFrame()\n",
    "    \n",
    "    # You have to request data backwards in time ...\n",
    "    while start < stop:\n",
    "        data = get_data_chunk(url, token, device[0], key, start, stop, 100000)\n",
    "        #print(data)\n",
    "        if key not in data:\n",
    "            break;\n",
    "        \n",
    "        #print(f\"{key}: Loaded {len(data[key])} points\")\n",
    "        t = pd.DataFrame.from_records(data[key])\n",
    "        #t['Timestamp'] = t['ts']\n",
    "        #pprint(t['ts'])\n",
    "        t['ts'] = (pd.to_datetime(t['ts'],unit='ms'))        \n",
    "        t.set_index('ts', inplace=True)\n",
    "        \n",
    "        t.rename(columns={'value': key}, inplace=True)\n",
    "        p = p._append(t)\n",
    "\n",
    "        # Update \"new\" stop time\n",
    "        stop = data[key][-1]['ts'] - 1\n",
    "    totalLength += len(p)\n",
    "    #print(f\"Total Length: {totalLength}\")\n",
    "    return p\n",
    "\n",
    "def outputCSV(devices):\n",
    "    global totalLength\n",
    "    final_df = pd.DataFrame()\n",
    "    for device in devices:\n",
    "        #print(f\"Downloading DEVICE: {device[0]} data\");\n",
    "        #print(device)\n",
    "        p = pd.DataFrame()\n",
    "        for key in keys:\n",
    "            #print(f\"info: Pulling {key}...\");\n",
    "            tempin = get_data(config['server'], token, device, key, startTS, endTS)            \n",
    "            if(len(tempin)>0):                \n",
    "                p = pd.concat([p,tempin], axis=1)\n",
    "        p['Entity Name'] = device[1]\n",
    "        p['dev_eui'] = device[2]\n",
    "        p.reset_index(drop=False)\n",
    "        #p_new_index = p.assign(**{'Timestamp': p.index})        \n",
    "        if(len(p)):\n",
    "            final_df = pd.concat([final_df,p])\n",
    "        \n",
    "    # Create Time Strings\n",
    "    # Convert to nanoseconds for pandas.to_datetime\n",
    "    start_timestamp_ns = startTS * 1000000\n",
    "    end_timestamp_ns = endTS * 1000000\n",
    "    \n",
    "    # Convert timestamp to datetime object\n",
    "    start_dt = pd.to_datetime(start_timestamp_ns, unit='ns')\n",
    "    end_dt = pd.to_datetime(end_timestamp_ns, unit='ns')\n",
    "    \n",
    "    # Format datetime string as yyyy-mm-dd-HH-MM\n",
    "    start_formatted_string = start_dt.strftime('%Y-%m-%d-%H-%M')\n",
    "    end_formatted_string = end_dt.strftime('%Y-%m-%d-%H-%M')\n",
    "    df_order = [\"Entity Name\",\"data_soil_moisture1\",\"data_soil_moisture2\",\"data_soil_moisture3\",\"data_soil_moisture4\",\"data_tem1\",\"data_tem2\",\"data_tem3\",\"data_tem4\",\"data_tem5\",\"data_tem6\",\"data_tem7\",\"dev_eui\"]\n",
    "    final_df = final_df.reindex(columns=df_order)\n",
    "    final_df1 = final_df.sort_values(by='ts')\n",
    "    \n",
    "    # Get current time\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    # Format time string (hours and minutes)\n",
    "    formatted_time = now.strftime(\"%H-%M\")\n",
    "    final_df1.to_csv(f\"./Raw_data/data-{end_formatted_string}.csv\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "def getDeviceCredentialsByDeviceId(deviceID = 0):\n",
    "    url = config['server']+'/device/'+deviceID+'/credentials'\n",
    "    resp = requests.get(url,headers=TBheaders)\n",
    "    responseList = resp.json()\n",
    "    #pprint(responseList)\n",
    "    return responseList['credentialsID']\n",
    "\n",
    "def getDeviceServerAttributes(deviceID = 0):\n",
    "    if deviceID == 0:\n",
    "        while(deviceID == 0):\n",
    "            try:\n",
    "                deviceID = input(\"Enter device ID: \")\n",
    "            except:\n",
    "                print(\"Invalid DeviceID\")\n",
    "    url = config['server']+'/plugins/telemetry/DEVICE/'+deviceID+'/values/attributes'\n",
    "    #pprint(url)\n",
    "    #pprint(TBheaders)\n",
    "    xresp = requests.get(url,headers=TBheaders)\n",
    "    #pprint(xresp)\n",
    "    #pprint(resp.content())\n",
    "    #print(xresp.text())\n",
    "    responseList = xresp.json()\n",
    "    #pprint(responseList)\n",
    "    #return responseList['credentialsID']\n",
    "\n",
    "\n",
    "# ** getting token for the request                                                                                         **\n",
    "print(\"Server: \",config['server'])\n",
    "token = login(config['server'], config['username'], config['password']);\n",
    "print(f\"Token: {token}\")\n",
    "TBheaders={ 'Accept': '*/*', 'X-Authorization': f\"Bearer {token}\" }\n",
    "\n",
    "\n",
    "\n",
    "# Create a datetime object representing the local date and time\n",
    "# Year, Month, Day, Hour, Minute\n",
    "today_dt = datetime.datetime.now()\n",
    "start = datetime.datetime.now()+ relativedelta(months=-2)\n",
    "\n",
    "start_dt = datetime.datetime(start.year, start.month, start.day, 18, 0)\n",
    "end_dt = datetime.datetime(today_dt.year, today_dt.month, today_dt.day, 6, 00)\n",
    "print (start_dt, end_dt)\n",
    "\n",
    "# Convert to a specific time zone (e.g., UTC)\n",
    "start_tz_utc = pytz.timezone(\"UTC\")\n",
    "start_dt_utc = start_tz_utc.localize(start_dt)\n",
    "end_tz_utc = pytz.timezone(\"UTC\")\n",
    "end_dt_utc = end_tz_utc.localize(end_dt)\n",
    "\n",
    "# Extract the Unix timestamp\n",
    "startTS = int(start_dt_utc.timestamp())*1000\n",
    "endTS = int(end_dt_utc.timestamp())*1000\n",
    "\n",
    "# Use for relative time frames\n",
    "#startTS = int((datetime.now() - timedelta(days=30)  - datetime(1970, 1, 1)).total_seconds() * 1000) # 30 days ago\n",
    "#endTS = int((datetime.datetime.utcnow() - datetime.datetime(1970, 1, 1)).total_seconds() * 1000) # now\n",
    "\n",
    "print(startTS, endTS)\n",
    "\n",
    "\n",
    "\n",
    "# ** customer ID for the request                                                                                            **\n",
    "# getCustomerDevices(custID, textSearch=None):\n",
    "# 7576b020-ecae-11ec-b72b-5dd76ca52a2b = Cherkhauer Customer ID\n",
    "# ABE-DRAGINO-GROPOINT-CHERKHAUER = Devices with names beginning with \"ABE-DRAGINO-GROPOINT-CHERKHAUER\"\n",
    "devices = getCustomerDevices(\"7576b020-ecae-11ec-b72b-5dd76ca52a2b\",\"ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE\")\n",
    "pprint(devices)\n",
    "\n",
    "totalLength = 0\n",
    "# keys to retrieve\n",
    "#keys = [\"data_TempC_SHT\",\"data_Hum_SHT\"]\n",
    "#keys = [\"data_ambient_temperature\",\"data_input1_frequency\",\"data_input1_frequency_to_moisture\",\"data_Input2_voltage\",\"data_Input2_voltage_to_temp\",\"data_light_intensity\",\"data_relative_humidity\"]\n",
    "keys = [\"data_soil_moisture1\",\"data_soil_moisture2\",\"data_soil_moisture3\",\"data_soil_moisture4\",\"data_tem1\",\"data_tem2\",\"data_tem3\",\"data_tem4\",\"data_tem5\",\"data_tem6\",\"data_tem7\"]\n",
    "\n",
    "outputCSV(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa92aa9",
   "metadata": {},
   "source": [
    "# Initial data reading process\n",
    "***\n",
    "## readraw_data Function\n",
    "\n",
    "The `readraw_data` function is used to read raw data from a CSV file and parse it into a pandas DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `destination`: The path where the output file will be saved.\n",
    "- `filename`: The name of the CSV file to be read.\n",
    "- `foutname`: The name of the output file.\n",
    "- `st_date`: The start date for the data to be read.\n",
    "- `ed_date`: The end date for the data to be read.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `raw_data`: A pandas DataFrame that contains the data read from the CSV file.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by using the pandas `read_csv` function to read the CSV file. It specifies the delimiter as ';' and parses the 'Timestamp' column as dates. It also specifies the data types for the soil moisture columns to be float64.\n",
    "\n",
    "The function then returns the DataFrame.\n",
    "***\n",
    "## station_data_clean Function\n",
    "\n",
    "The `station_data_clean` function is used to clean the data for a specific station.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `raw_data`: A pandas DataFrame that contains the raw data to be cleaned.\n",
    "- `station`: An integer that represents the station ID.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `station_data`: A pandas DataFrame that contains the cleaned data for the specified station.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by filtering the raw data for the specified station. It then performs any necessary cleaning operations, such as removing missing values, outliers, or incorrect data.\n",
    "\n",
    "The function then returns the cleaned data for the specified station.\n",
    "\n",
    "Please note that the actual code for the `station_data_clean` function is not provided, so the parameters and functionality are assumed based on typical usage. If you provide the actual code of the `station_data_clean` function, I can give a more accurate explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13944bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import statistics as stats\n",
    "import os\n",
    "\n",
    "\n",
    "today = dt.date.today()\n",
    "# today = datetime(2009, 7, 6, 0, 0)\n",
    "strtoday = today.strftime(\"%Y%m%d\")\n",
    "print ('Today is :: ',today)\n",
    "\n",
    "\n",
    "\n",
    "def readraw_data(destination, filename, foutname, st_date, ed_date):\n",
    "    \n",
    "    # open the file\n",
    "    raw_data = pd.read_csv(filename,delimiter=';', parse_dates=['Timestamp'],\n",
    "                          dtype={'data_soil_moisture1':np.float64,\n",
    "                                 'data_soil_moisture2':np.float64,\n",
    "                                 'data_soil_moisture3':np.float64,\n",
    "                                 'data_soil_moisture4':np.float64},\n",
    "                          na_values=['Invalid data']\n",
    "                          )\n",
    "    raw_columns = raw_data.columns.tolist()\n",
    "    # raw_data['Timestamp'] = pd.to_datetime(raw_data['Timestamp'])\n",
    "    # drop the temperature data from the list\n",
    "    for i in range(len(raw_columns)):\n",
    "        if 'tem' in raw_columns[i] or 'dev' in raw_columns[i]:\n",
    "            raw_data = raw_data.drop(columns=raw_columns[i])\n",
    "            \n",
    "    raw_data.set_index(['Timestamp'])\n",
    "    print(raw_data.info())\n",
    "    \n",
    "    # change the name of the Entity Name column\n",
    "    raw_data['Entity Name'] = raw_data['Entity Name'].str.replace('ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-','')\n",
    "    stationlist = sorted(raw_data['Entity Name'].unique())\n",
    "        \n",
    "    \n",
    "    # after checking the null values, filter the data\n",
    "    clean_df = pd.DataFrame(columns = ['Station','Layer1', 'Layer2', 'Layer3', 'Layer4'])\n",
    "    for station in stationlist:\n",
    "        # clean the data according to the cleaning procedure\n",
    "        sample_df = station_data_clean(destination, raw_data, station)\n",
    "        \n",
    "        clean_df = pd.concat([clean_df,sample_df])\n",
    "        \n",
    "    clean_df.to_csv(destination+foutname,index=True)\n",
    "    \n",
    "    # convert headers only with numbers\n",
    "    raw_columns = raw_data.columns.tolist()\n",
    "    \n",
    "    # group the dataframe and turn them into another dataframe with 'Entity Name' as columns.\n",
    "    # this step should be done by layer.\n",
    "    \n",
    "\n",
    "    return raw_data, raw_columns, clean_df, stationlist, raw_columns\n",
    "\n",
    "\n",
    "\n",
    "# ********************************************************************************************************************\n",
    "# * this is the function that goes into readraw_data function.                                                       *\n",
    "# ********************************************************************************************************************\n",
    "def station_data_clean(destination, raw_data, station):\n",
    "    global start_date, end_date\n",
    "    '''\n",
    "    This is cleaning for station data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_data : dataframe\n",
    "        dataframe of the raw_data, 'raw_data' in this script\n",
    "    station : string\n",
    "        this is number of station. 4 digit number filled with zero from left\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_df : dataframe\n",
    "        this cleaned data after process\n",
    "\n",
    "    '''\n",
    "    sample_df = raw_data[raw_data['Entity Name']==station]\n",
    "    sample_df = sample_df.drop(['Entity Name'], axis=1)\n",
    "    sample_df = sample_df.set_index('Timestamp')\n",
    "    sample_df = sample_df.set_axis(['Layer1', 'Layer2', 'Layer3', 'Layer4'], axis=1)\n",
    "    \n",
    "    # clean the data by time of interest\n",
    "    # datetime range should start by 18:00 // end by 6:00 for better analysis for everyday\n",
    "    start_date = dt.datetime(st_date[0], st_date[1], st_date[2], 0, 0, 0)\n",
    "    end_date = dt.datetime(ed_date[0], ed_date[1], ed_date[2], 6, 0, 0)\n",
    "    sample_df = sample_df[(sample_df.index > start_date) & (sample_df.index < end_date)]\n",
    "    \n",
    "    # according to the number of the data length,\n",
    "    # if it is more than 1, the dataframe will be made\n",
    "    # if it is 0, below process will be skipped\n",
    "    totnum = len(sample_df)\n",
    "    if totnum > 0:    \n",
    "        print ('\\n\\n')\n",
    "        txt = ' raw_data info for station '+station+' '\n",
    "        print(txt.center(60,'='),end='\\n')\n",
    "        \n",
    "        # if value is null value from beginning\n",
    "        print(' NaN values info '.center(60,':'))\n",
    "        na_df = sample_df[sample_df.isna().any(axis=1)]\n",
    "        sample_df = sample_df.dropna()\n",
    "        print('NaN values are ::',len(na_df),'out of',totnum,'\\nerror rate:',round(len(na_df)/totnum*100,2),'%',end='\\n\\n')\n",
    "        \n",
    "        # if value is out of range\n",
    "        sample_df[(sample_df>=100.0) | (sample_df<=0.0)] = np.nan\n",
    "        outrange_df = sample_df[sample_df.isna().any(axis=1)]\n",
    "        sample_df = sample_df.dropna()\n",
    "        print('Out of range values are ::',len(outrange_df),'out of',totnum,'\\nerror rate:',round(len(outrange_df)/totnum*100,2),'%',end='\\n\\n')\n",
    "        \n",
    "        print(' Data Describe '.center(60,':'),end='\\n\\n')\n",
    "        print(sample_df.dtypes, end='\\n\\n')\n",
    "        print(sample_df.describe())\n",
    "        \n",
    "        # resampling in 30 min frequency\n",
    "        sample_df = sample_df.resample('30min').mean()\n",
    "        \n",
    "        # make boxplot per layer\n",
    "        boxplot = sample_df.boxplot(column=['Layer1', 'Layer2', 'Layer3'],figsize=(8,4), ylabel='Soil Moisture (%)')\n",
    "        plt.title(int(station))\n",
    "        plt.savefig(destination+'station_data_'+station+'.png',dpi=600)\n",
    "        plt.show()\n",
    "        \n",
    "        # add station code back\n",
    "        sample_df['Station'] = int(station)\n",
    "    \n",
    "        print('='*60)\n",
    "        \n",
    "    else:\n",
    "        txt = station+' '\n",
    "        print(txt.center(60,'='),end='\\n\\n')\n",
    "    '''\n",
    "    # save station dataframe as csv file == turned off for cal_deficit\n",
    "    try:\n",
    "        sample_df.to_csv(destination+'station_data/'+station+'_cleaned.csv',sep=',')\n",
    "        print('Transmitter data exported.')\n",
    "    except:\n",
    "        print('Transmitter data export failed.')\n",
    "    '''\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d74da",
   "metadata": {},
   "source": [
    "## Rainfinder Function\n",
    "\n",
    "The `rainfinder` function is used to identify significant rain events in a given dataset. The function is designed to analyze weather data and detect periods of rainfall based on certain conditions or thresholds.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `data`: A pandas DataFrame that contains the weather data to be analyzed. The DataFrame should be indexed by 'Timestamp and Station'.\n",
    "- `station`: An integer that represents the station ID.\n",
    "- `header`: A list that contains the headers of the data.\n",
    "- `threshold_moist`: An integer that represents the threshold of soil moisture difference to decide if it was a significant rain event or not.\n",
    "- `raintimestep`: An integer that represents the number of time steps to consider for the rolling window.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `bumplist2`: A list of lists. Each inner list represents a date (in the format [year, month, day]) when a significant rain event (or \"bump\") was detected.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by first filtering the data for the specified station and removing any missing values. It then calculates the difference in soil moisture between each time step and filters out the time steps where the difference is greater than the specified threshold.\n",
    "\n",
    "The function then uses a rolling window to find the minimum and maximum soil moisture values within each window. It calculates the gap between the min and max values and filters out the time steps where the gap is greater than the threshold.\n",
    "\n",
    "The function then organizes the dates of these significant rain events, removes duplicates, and sorts the list. The sorted list of dates is returned.\n",
    "\n",
    "The function uses pandas for data manipulation and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b747502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rainfinder(data, station, header, threshold_moist, raintimestep):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        organized dataframe / index is 'Timestamp and Station'\n",
    "    stationlist : list\n",
    "        list of the stations\n",
    "    header : list\n",
    "        header of the data\n",
    "    threshold_moist : int\n",
    "        threshold of soil moisture difference to decide it was huge rain event or not\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data_dur_all : list of dataframe\n",
    "        this is dataframes with time range of interest\n",
    "        dataframe order is accordance with order of station list (stationlist)\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: remove stationlist for the final version\n",
    "    text = station +' :: '+target+'  Rainfinder'\n",
    "    print (text.center(60,':'))\n",
    "\n",
    "    data_dur = data[data['Station']==int(station)]\n",
    "    data_dur = data_dur.dropna()\n",
    "    \n",
    "        \n",
    "    # make a rolling window column from interpolation\n",
    "    # data_dur[target] = data_dur[target].interpolate()\n",
    "    print (station,'|','LEN =', len(data_dur))\n",
    "    \n",
    "    # find differene between each time step\n",
    "    temp = data_dur[target].diff()\n",
    "    filtered_temp = temp.to_frame(name=target).query('{target} > @threshold_moist'.format(target = target))\n",
    "    \n",
    "    # find biggest difference within 3 hours (6 timesteps) == bump\n",
    "    data_dur['min_'+target] = data_dur[target].rolling(window=raintimestep).min()\n",
    "    data_dur['max_'+target] = data_dur[target].rolling(window=raintimestep).max()\n",
    "    data_dur['gap_'+target] = data_dur['max_'+target] - data_dur['min_'+target]\n",
    "    filtered_temp = data_dur.query('gap_{target} > @threshold_moist'.format(target = target))\n",
    "    #print (filtered_temp)\n",
    "\n",
    "    # organizing the date of bump\n",
    "    bumplist = filtered_temp.index.to_list()        \n",
    "    for i in range(len(bumplist)):\n",
    "        # make a list of date with the bump in the form of [year, month, day]\n",
    "        bumplist[i] = [bumplist[i].year, bumplist[i].month, bumplist[i].day]\n",
    "    # remove duplicates in bumplist\n",
    "    bumplist2 = list(set(map(tuple, bumplist)))\n",
    "    # sort the list and convert tuples into list\n",
    "    bumplist2 = sorted(bumplist2)\n",
    "    for i in range(len(bumplist2)):\n",
    "        bumplist2[i] = list(bumplist2[i])\n",
    "\n",
    "    print ('Searched ', len(bumplist2), 'days with bump events')\n",
    "    print (bumplist2)\n",
    "\n",
    "    return bumplist2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70026a60",
   "metadata": {},
   "source": [
    "\n",
    "## FCfinder Function\n",
    "\n",
    "The `FCfinder` function is used to find the field capacity of soil after a given number of days from a specified start date. Field capacity is the amount of soil moisture or water content held in the soil after excess water has drained away and the rate of downward movement has decreased. This capacity is reached within 2â€“3 days after rain or irrigation in typical soil conditions.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `data`: A pandas DataFrame that contains the data to be analyzed. The DataFrame should be indexed by 'Timestamp and Station'.\n",
    "- `station`: An integer that represents the station ID.\n",
    "- `header`: A list that contains the headers of the data.\n",
    "- `start_date`: A list that contains the start date of the data search in the format [year, month, day].\n",
    "- `search_days`: An integer that represents the number of days to search for field capacity.\n",
    "- `search_range`: An integer that represents the range (in hours) for moving field capacity.\n",
    "- `search_slope`: A float that represents the slope for searching.\n",
    "- `threshold_hour`: An integer that represents the threshold hours that remains flat soil moisture behavior to confirm it is a field capacity point.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `data_dur_all`: A list of pandas DataFrames. Each DataFrame contains a time range of interest. The order of the DataFrames corresponds to the order of the station list.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by iterating over a specified number of days from the start date. For each day, it identifies a time range around midnight and checks if the standard deviation of the soil moisture within this time range is less than the specified slope. If it is, the function considers this as a constant moisture behavior and records the date and the mean soil moisture value. The function then plots the soil moisture and the field capacity points, saves the plot as a PNG file, and writes the field capacity records to a CSV file.\n",
    "\n",
    "The function uses a rolling window to smooth the soil moisture data and uses the standard deviation to identify constant moisture behavior. It also uses matplotlib for plotting and os for file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed02d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCfinder(data, station, header, start_date, search_days, search_range, search_slope, threshold_hour):\n",
    "    global searched, precip_search1, precip_search2, precip_delta, precip_max\n",
    "\n",
    "    # print ('Station',station, 'Date',start_date)\n",
    "    # important == rl means rolling window. if you don't want, remove it.\n",
    "    search_target = 'rl_'+target\n",
    "\n",
    "    data_dur_all = []\n",
    "\n",
    "    #data.plot(x = data.index,  y = target, data = data[data['Station']==station])\n",
    "\n",
    "    data_dur = data[data['Station']==int(station)]\n",
    "    \n",
    "        \n",
    "    # make a rolling window column from interpolation\n",
    "    data_dur[target] = data_dur[target].interpolate()\n",
    "    # print (data_dur[target])\n",
    "    data_dur['rl_'+target] = data_dur[target].rolling(window=3).mean()\n",
    "    \n",
    "\n",
    "    '''\n",
    "    From here, we need to find out point where soil moisture value is consistent for 2 hours (threshold_hour)\n",
    "    And consistent variation for this hour is < 1% (search_slope)\n",
    "    Searching range will be +- 4hrs from midnight (search_range)\n",
    "    '''\n",
    "    # set the database only for the search_target\n",
    "    target_data = data_dur[search_target]\n",
    "    \n",
    "    # find 00:00 hour of everyday within TOI (7 days)\n",
    "    dayrange = search_days\n",
    "    \n",
    "    # this list is for the searched fc dates\n",
    "    searched_list = []\n",
    "    \n",
    "    # searching field capacity (daily)\n",
    "    for days in range(1,dayrange+1):\n",
    "        # Going to find the field capacity after given days from the start_date\n",
    "        st_datetime = dt.datetime(start_date[0], start_date[1], start_date[2], 6, 0, 0)\n",
    "\n",
    "        # stamp2 is 4 hours before the midnight of the stamp1\n",
    "        # stamp3 is 4 hours after the midnight of the stamp2\n",
    "        # add 'days' to search every daily step\n",
    "        stamp1 = st_datetime + dt.timedelta(days=days)\n",
    "        stamp1 = stamp1.replace(hour=0, minute=0, second=0)\n",
    "        # print(stamp1)\n",
    "        \n",
    "        # step 1. select the time range of interest\n",
    "        # once you find the time, you will search there is constant soil moisture behavior or not\n",
    "        # set the searching time\n",
    "        stamp2 = stamp1 - dt.timedelta(hours=search_range)\n",
    "        stamp3 = stamp1 + dt.timedelta(hours=search_range)\n",
    "        # print(' SM ',stamp2, '--' ,stamp3)\n",
    "        \n",
    "        # step 2. set the searching range\n",
    "        searched = target_data[(target_data.index > stamp2) & (target_data.index < stamp3)]\n",
    "        # searched = data_search\n",
    "            \n",
    "        try:\n",
    "            # print('STD::',round(stats.stdev(searched),2), 'MEAN::',round(stats.mean(searched),2))\n",
    "            \n",
    "            # if standard deviation does not exceeds 'search_slope',\n",
    "            # this will be regarded as constant moisture behavior\n",
    "            if stats.stdev(searched) < search_slope:\n",
    "                searched_list.append([stamp1, stats.mean(searched)])\n",
    "                FC_searched = True\n",
    "            else:\n",
    "                # print(stamp2,'has no moving field capacity.', end='\\n\\n')\n",
    "                FC_searched = False\n",
    "        except:\n",
    "            # print(station, stamp2, 'has no data.', end='\\n\\n')\n",
    "            FC_searched = False\n",
    "        \n",
    "        if FC_searched == True:\n",
    "            print ('Field Capacity is found for',str(start_date[0])+'-'+str(start_date[1])+'-'+str(start_date[2]),':',searched_list[0][0].strftime('%Y-%m-%d'),round(searched_list[0][1],3))\n",
    "\n",
    "        \n",
    "            # list of searched field capacity\n",
    "            searched_dates = [i[0] for i in searched_list]\n",
    "            searched_values = [i[1] for i in searched_list]\n",
    "            \n",
    "            # if field capacity is found, draw graph\n",
    "            if len(searched_dates) > 0:\n",
    "                # draw overlaying graph for the soil moisture and field capacity\n",
    "                # soil moisture is blue line and field capacity is red dot            \n",
    "                # data_dur = data_dur.reset_index()\n",
    "                df_subset = data_dur[(data_dur.index < stamp1+dt.timedelta(days=1)) & (data_dur.index > st_datetime)]\n",
    "                print(df_subset)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(12, 5))\n",
    "                # ax = data_dur.plot(y=target, label = 'Observed values', figsize=(15,5))\n",
    "                ax = plt.plot(df_subset.index, df_subset[target], label = 'Observed values')\n",
    "\n",
    "                # ax.set_xlim(stamp2, stamp3)\n",
    "                plt.scatter(x=searched_dates, y=searched_values, label = 'Moving FC points', marker='s', c='r', s=100)\n",
    "                plt.legend()\n",
    "                plt.gcf().autofmt_xdate()\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Soil Moisture (%)')\n",
    "                plt.title('Field Capacity for '+str(station)+' after '+st_datetime.strftime('%Y-%m-%d'))\n",
    "\n",
    "                # save graph for the soil moisture and field capacity\n",
    "                graph_dest = os.getcwd()+'/fc_graphs/'+st_datetime.strftime('%Y-%m-%d')\n",
    "                if not os.path.exists(graph_dest):\n",
    "                    os.makedirs(graph_dest)\n",
    "                plt.savefig(graph_dest+'/'+str(station)+'_'+stamp1.strftime('%Y-%m-%d')+'.png',dpi=600)\n",
    "                plt.show()\n",
    "                print('Graph is saved.')\n",
    "            \n",
    "            # save field capacity records in csv form\n",
    "            data_dur_all.append(data_dur)\n",
    "            \n",
    "            \n",
    "            # save field capacity records in csv form\n",
    "            with  open(os.getcwd()+'/'+'fc_record.csv', 'a+') as f:\n",
    "                for item in searched_list:\n",
    "                    f.write(str(station)+','+item[0].strftime('%Y-%m-%d')+','+str(round(item[1],2))+'\\n')\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return data_dur_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FCfinder2(data, station, header, start_date, search_days, search_range, search_slope, threshold_hour):\n",
    "    global searched, precip_search1, precip_search2, precip_delta, precip_max\n",
    "    \"\"\"\n",
    "    This function finds the field capacity of the soil given a dataset, station, header, start date, \n",
    "    number of search days, search range, search slope, and threshold hour.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Target variable\n",
    "    target = header\n",
    "    \n",
    "    # Interpolating missing values and creating a rolling window column\n",
    "    data_dur = data[data['Station'] == int(station)]\n",
    "    data_dur[target] = data_dur[target].interpolate()\n",
    "    data_dur['rl_' + target] = data_dur[target].rolling(window=3).mean()\n",
    "    \n",
    "    # Defining search target\n",
    "    search_target = 'rl_' + target\n",
    "    \n",
    "    # Data duration list\n",
    "    data_dur_all = []\n",
    "    \n",
    "    # Searching field capacity for each day in the range\n",
    "    for days in range(1, search_days + 1):\n",
    "        st_datetime = dt.datetime(start_date[0], start_date[1], start_date[2], 6, 0, 0)\n",
    "        stamp1 = st_datetime + dt.timedelta(days=days)\n",
    "        stamp1 = stamp1.replace(hour=0, minute=0, second=0)\n",
    "        \n",
    "        # Setting search range\n",
    "        stamp2 = stamp1 - dt.timedelta(hours=search_range)\n",
    "        stamp3 = stamp1 + dt.timedelta(hours=search_range)\n",
    "        searched = data_dur[(data_dur.index > stamp2) & (data_dur.index < stamp3)]\n",
    "        \n",
    "        try:\n",
    "            # Checking for constant moisture behavior\n",
    "            if stats.stdev(searched) < search_slope:\n",
    "                searched_list.append([stamp1, stats.mean(searched)])\n",
    "                FC_searched = True\n",
    "            else:\n",
    "                FC_searched = False\n",
    "        except:\n",
    "            FC_searched = False\n",
    "        \n",
    "        if FC_searched:\n",
    "            print('Field Capacity is found for', str(start_date[0]) + '-' + str(start_date[1]) + '-' + str(start_date[2]), ':', searched_list[0][0].strftime('%Y-%m-%d'), round(searched_list[0][1], 3))\n",
    "        \n",
    "            # Graphing\n",
    "            searched_dates = [i[0] for i in searched_list]\n",
    "            searched_values = [i[1] for i in searched_list]\n",
    "            \n",
    "            if len(searched_dates) > 0:\n",
    "                df_subset = data_dur[(data_dur.index < stamp1 + dt.timedelta(days=1)) & (data_dur.index > st_datetime)]\n",
    "                fig, ax = plt.subplots(figsize=(12, 5))\n",
    "                ax = plt.plot(df_subset.index, df_subset[target], label='Observed values')\n",
    "                plt.scatter(x=searched_dates, y=searched_values, label='Moving FC points', marker='s', c='r', s=100)\n",
    "                plt.legend()\n",
    "                plt.gcf().autofmt_xdate()\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Soil Moisture (%)')\n",
    "                plt.title('Field Capacity for ' + str(station) + ' after ' + st_datetime.strftime('%Y-%m-%d'))\n",
    "                \n",
    "                graph_dest = os.getcwd() + '/fc_graphs/' + st_datetime.strftime('%Y-%m-%d')\n",
    "                if not os.path.exists(graph_dest):\n",
    "                    os.makedirs(graph_dest)\n",
    "                plt.savefig(graph_dest + '/' + str(station) + '_' + stamp1.strftime('%Y-%m-%d') + '.png', dpi=600)\n",
    "                plt.show()\n",
    "                print('Graph is saved.')\n",
    "            \n",
    "            data_dur_all.append(data_dur)\n",
    "            \n",
    "            # Saving field capacity records in CSV form\n",
    "            with open(os.getcwd() + '/' + 'fc_record.csv', 'a+') as f:\n",
    "                for item in searched_list:\n",
    "                    f.write(str(station) + ',' + item[0].strftime('%Y-%m-%d') + ',' + str(round(item[1], 2)) + '\\n')\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return data_dur_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562fcbf",
   "metadata": {},
   "source": [
    "# About the 'Defecit Calc' function\n",
    "## About\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3062411",
   "metadata": {},
   "source": [
    "# Below is implementation Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91870e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strtoday' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m filename \u001b[38;5;241m=\u001b[39m destination \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw_data/Calc_def_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ! filename should be changed to the file name of the downloaded data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m foutname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC_data\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mstrtoday\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# set the time of interest and location [year, month, day]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#  this is for the data of soil moisture. = for the date of the analysis\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# for regular \u001b[39;00m\n\u001b[1;32m     22\u001b[0m ed_date \u001b[38;5;241m=\u001b[39m [dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39myear, dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mmonth, dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mday]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strtoday' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "Implementation Block\n",
    "\n",
    "set up the variables for running the programs\n",
    "you will set the destination of the files (directory)\n",
    "you will set the date of your interest.\n",
    "usually, it is set to today's date\n",
    "\n",
    "'''\n",
    "\n",
    "destination = ''\n",
    "filename = destination + 'Raw_data/Calc_def_test.csv'\n",
    "# ! filename should be changed to the file name of the downloaded data\n",
    "foutname = 'FC_data'+strtoday+'.csv'\n",
    "\n",
    "# set the time of interest and location [year, month, day]\n",
    "#  this is for the data of soil moisture. = for the date of the analysis\n",
    "# for regular \n",
    "ed_date = [dt.date.today().year, dt.date.today().month, dt.date.today().day]\n",
    "# end date will be \n",
    "st_datetime = dt.datetime(ed_date[0], ed_date[1], ed_date[2], 6, 0, 0) - dt.timedelta(days=360)\n",
    "st_date = [st_datetime.year, st_datetime.month, st_datetime.day]\n",
    "\n",
    "raw_data, raw_columns, clean_df, stationlist, raw_columns = readraw_data(destination, filename, foutname, st_date, ed_date)\n",
    "\n",
    "\n",
    "''' \n",
    "This part should be done per station [stationlist]\n",
    "'''\n",
    "# stationlist = ['0111','0112','0114']\n",
    "print ('Station List ::',stationlist)\n",
    "\n",
    "# this is the target layer for the analysis. 1=layer 1\n",
    "# variable set for rainfinder\n",
    "target = 'Layer2'\n",
    "threshold_moist = 15    # threshold of soil moisture difference to decide it was huge rain event or not\n",
    "raintimestep = 12 # 1 = 1/2 hour // 12 = 6 hours\n",
    "\n",
    "# variable set for FCfinder\n",
    "search_days = 14 # days\n",
    "search_range = 5 # hours\n",
    "search_slope = 0.07 # unit is fraction for standard deviation of the soil moisture behavior\n",
    "threshold_hour = 4 # hours with consistent soil moisture behavior\n",
    "\n",
    "\n",
    "for station in stationlist:\n",
    "    print('\\n\\n')\n",
    "    text = station +' :: '+target\n",
    "    print (text.center(60,'='))\n",
    "    bumplist = rainfinder(clean_df, station, raw_columns, threshold_moist, raintimestep)\n",
    "    for start_date in bumplist:\n",
    "        FCfinder2(clean_df, station, raw_columns, start_date, search_days, search_range, search_slope, threshold_hour)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_dur_all = FCfinder(clean_df, stationlist, raw_columns, search_range, search_slope, threshold_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e74af",
   "metadata": {},
   "source": [
    "# About the 'Defecit calc' function\n",
    "## About\n",
    "***\n",
    "### Job in the script\n",
    "This is for calcuating soil moisture defecit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df890819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root depth\n",
    "root=24\n",
    "\n",
    "df2.loc[df2['soil_moisture1'] < fcdf.fc1, 'd1']=fcdf.fc1-df2['soil_moisture1']\n",
    "df2.loc[df2['soil_moisture2'] < fcdf.fc2, 'd2']=fcdf.fc2-df2['soil_moisture2']\n",
    "df2.loc[df2['soil_moisture3'] < fcdf.fc3, 'd3']=fcdf.fc3-df2['soil_moisture3']\n",
    "df2.loc[df2['soil_moisture4'] < fcdf.fc4, 'd4']=fcdf.fc4-df2['soil_moisture4']\n",
    "\n",
    "if root < 6:\n",
    "    df2['Deficit'] = (df2['d1'])*root\n",
    "elif root < 12:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*(root-6)\n",
    "elif root < 18:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*6 + (df2['d3'])*(root-12)\n",
    "else:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*6 + (df2['d3'])*6 + (df2['d4'])*(root-18)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
