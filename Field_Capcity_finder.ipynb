{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e051e6ed",
   "metadata": {},
   "source": [
    "# Field Capacity Finder from the soil moisture\n",
    "\n",
    "`[Run All] is mendatory`\n",
    "\n",
    "## From the soil moisture data\n",
    "***\n",
    "### Data description\n",
    "- data is being collected from the F70 field.\n",
    "- soil moisture is in unit of %, **not cleaned**\n",
    "- this script includes cleaning function\n",
    "- this script designed to be used in daily basis.\n",
    "- from the soil moisture monitoring website\n",
    "- example data is 'Calc_def_test.csv'\n",
    "\n",
    "- [soil moisture data](https://things.iot.ag.purdue.edu:8080/dashboard/dc56c5a0-ee3e-11ec-b72b-5dd76ca52a2b?publicId=a914a590-ecae-11ec-b72b-5dd76ca52a2b) input data for the script will be downloaded directly\n",
    "> - soil moisture data is from Purdue AgIT server that collects the field data through LoRaWan network\n",
    "> - the data should be downloaded from the 'Summary Data Table' tab\n",
    "> \n",
    "- [weather data] input data for the script will be downloaded directly\n",
    "> - weather data is from Purdue Mesonet server and using ACRE station data\n",
    "> \n",
    "For downloading the data\n",
    "  - Summary data table\n",
    "  - from date1 to date2\n",
    "  - in csv form\n",
    "  - file name does not matter or add today's date at the end of the file name.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c384a2",
   "metadata": {},
   "source": [
    "### Version history\n",
    "***\n",
    "#### Ver 1.1\n",
    "- ver 1.1 branch made\n",
    "- Shared with co-author Dr.Chandra\n",
    "- Added Thingsboard API to the program\n",
    "- deficit report export function to [./Deficit_results/] (2024-05-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac09df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee0bedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:  https://things.iot.ag.purdue.edu:8080\n",
      "Token: eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ5YW5nMjMwOUBwdXJkdWUuZWR1IiwidXNlcklkIjoiNjRlOWZjYjAtZjc0ZS0xMWVlLWIzYmMtN2ZlNjliZjhkNDExIiwic2NvcGVzIjpbIkNVU1RPTUVSX1VTRVIiXSwic2Vzc2lvbklkIjoiOGExZDEwODctMjg5MS00MThjLTlmODQtZGY4N2NkYzMzOGQ5IiwiaXNzIjoidGhpbmdzYm9hcmQuaW8iLCJpYXQiOjE3MTU4OTA5MDgsImV4cCI6MTcxNTg5OTkwOCwiZmlyc3ROYW1lIjoiRG9uZ3Nlb2siLCJsYXN0TmFtZSI6IllhbmciLCJlbmFibGVkIjp0cnVlLCJpc1B1YmxpYyI6ZmFsc2UsInRlbmFudElkIjoiYWFjNjU1YTAtYWM2Mi0xMWVjLWFiYzgtMWYxYzA5NTgwZTY3IiwiY3VzdG9tZXJJZCI6Ijc1NzZiMDIwLWVjYWUtMTFlYy1iNzJiLTVkZDc2Y2E1MmEyYiJ9.NsitAfY_8lwcwQ7LTHS_0BcPTnvIjMHoY1fMgY_zuWzLMnHzWbn9VOFV9qoVzJhExZyUOPC82oWL0Juhc-dQmA\n",
      "2024-03-16 18:00:00 2024-05-16 06:00:00\n",
      "File Export Done.\n"
     ]
    }
   ],
   "source": [
    "# // this part is download the data from the website for 2 month.\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pytz\n",
    "import config\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "deviceList = []\n",
    "\n",
    "# ** set the configuration for the request                                                                                  **\n",
    "config = {\n",
    " 'username' : 'yang2309@purdue.edu', ### Insert your email address used by AgIT Thingsboard system\n",
    " 'password': 'dsya2002',  ### Insert your AgIT thingsboard password\n",
    " 'server' : 'https://things.iot.ag.purdue.edu:8080'\n",
    "}\n",
    "\n",
    "# ** defining the function to get the token for the request and setting the header for the request                          **\n",
    "def getCustomerDevices(custID, textSearch=None):\n",
    "    parameters = {        \n",
    "        'pageSize': 1000,\n",
    "        'page': 0,                \n",
    "    }\n",
    "    att_parms = {\n",
    "        'keys': 'dev_eui'\n",
    "    }\n",
    "    if(textSearch):\n",
    "        parameters.update({'textSearch': textSearch})\n",
    "    responseList = requests.get(f\"{config['server']}/api/customer/{custID}/devices\", headers=TBheaders,params= parameters).json()\n",
    "    #pprint(responseList)\n",
    "    list = []\n",
    "    for dev in responseList['data']:\n",
    "        #pprint(dev)\n",
    "        #print('------------------------------------------------------------------------------------------')\n",
    "        #'id': {'entityType': 'DEVICE', 'id': 'd49153a0-c868-11eb-95d8-09d06ef6a9a5'},\n",
    "        url = f\"{config['server']}/api/plugins/telemetry/DEVICE/{dev['id']['id']}/values/attributes\"\n",
    "        deviceResp = requests.get(url, headers=TBheaders,params= att_parms).json()\n",
    "        #print('------------------------------------------------------------------------------------------')\n",
    "        list.append([dev['id']['id'],dev['name'],deviceResp[0]['value']])\n",
    "    return list\n",
    "        \n",
    "\n",
    "def login(url, username, password):\n",
    "    # Log into ThingsBoard\n",
    "    return requests.post(f\"{url}/api/auth/login\", json={\n",
    "        \"username\": username,\n",
    "        \"password\": password\n",
    "    }).json()['token']\n",
    "\n",
    "def get_keys(device):\n",
    "    return requests.get(f\"{config['server']}/api/plugins/telemetry/DEVICE/{device}/keys/timeseries\",\n",
    "                 headers=TBheaders).json()\n",
    "def get_data_chunk(url, token, device, key, start, stop, limit):\n",
    "    #print([url, device, key, start, stop, limit])\n",
    "    return requests.get(f\"{url}/api/plugins/telemetry/DEVICE/{device}/values/timeseries\",\n",
    "             headers=TBheaders,\n",
    "            params= {\n",
    "                'keys': key,\n",
    "                'startTs': start,\n",
    "                'endTs': stop,\n",
    "                'limit': limit,\n",
    "                'agg': 'NONE'\n",
    "            }).json()\n",
    "\n",
    "def get_data(url, token, device, key, start, stop):\n",
    "    global totalLength\n",
    "    p = pd.DataFrame()\n",
    "    \n",
    "    # You have to request data backwards in time ...\n",
    "    while start < stop:\n",
    "        data = get_data_chunk(url, token, device[0], key, start, stop, 100000)\n",
    "        #print(data)\n",
    "        if key not in data:\n",
    "            break;\n",
    "        \n",
    "        #print(f\"{key}: Loaded {len(data[key])} points\")\n",
    "        t = pd.DataFrame.from_records(data[key])\n",
    "        #t['Timestamp'] = t['ts']\n",
    "        #pprint(t['ts'])\n",
    "        t['ts'] = (pd.to_datetime(t['ts'],unit='ms'))        \n",
    "        t.set_index('ts', inplace=True)\n",
    "        \n",
    "        t.rename(columns={'value': key}, inplace=True)\n",
    "        p = p._append(t)\n",
    "\n",
    "        # Update \"new\" stop time\n",
    "        stop = data[key][-1]['ts'] - 1\n",
    "    totalLength += len(p)\n",
    "    #print(f\"Total Length: {totalLength}\")\n",
    "    return p\n",
    "\n",
    "def outputCSV(devices):\n",
    "    global totalLength\n",
    "    final_df = pd.DataFrame()\n",
    "    for device in devices:\n",
    "        #print(f\"Downloading DEVICE: {device[0]} data\");\n",
    "        #print(device)\n",
    "        p = pd.DataFrame()\n",
    "        for key in keys:\n",
    "            #print(f\"info: Pulling {key}...\");\n",
    "            tempin = get_data(config['server'], token, device, key, startTS, endTS)            \n",
    "            if(len(tempin)>0):                \n",
    "                p = pd.concat([p,tempin], axis=1)\n",
    "        p['Entity Name'] = device[1]\n",
    "        p['dev_eui'] = device[2]\n",
    "        p.reset_index(drop=False)\n",
    "        #p_new_index = p.assign(**{'Timestamp': p.index})        \n",
    "        if(len(p)):\n",
    "            final_df = pd.concat([final_df,p])\n",
    "        \n",
    "    # Create Time Strings\n",
    "    # Convert to nanoseconds for pandas.to_datetime\n",
    "    start_timestamp_ns = startTS * 1000000\n",
    "    end_timestamp_ns = endTS * 1000000\n",
    "    \n",
    "    # Convert timestamp to datetime object\n",
    "    start_dt = pd.to_datetime(start_timestamp_ns, unit='ns')\n",
    "    end_dt = pd.to_datetime(end_timestamp_ns, unit='ns')\n",
    "    \n",
    "    # Format datetime string as yyyy-mm-dd-HH-MM\n",
    "    start_formatted_string = start_dt.strftime('%Y-%m-%d-%H-%M')\n",
    "    end_formatted_string = end_dt.strftime('%Y-%m-%d-%H-%M')\n",
    "    df_order = [\"Entity Name\",\"data_soil_moisture1\",\"data_soil_moisture2\",\"data_soil_moisture3\",\"data_soil_moisture4\",\"data_tem1\",\"data_tem2\",\"data_tem3\",\"data_tem4\",\"data_tem5\",\"data_tem6\",\"data_tem7\",\"dev_eui\"]\n",
    "    final_df = final_df.reindex(columns=df_order)\n",
    "    final_df1 = final_df.sort_values(by='ts')\n",
    "    \n",
    "    # Get current time\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    # Format time string (hours and minutes)\n",
    "    formatted_time = now.strftime(\"%H-%M\")\n",
    "    final_df1.to_csv(f\"./Raw_data/data-{end_formatted_string}.csv\")\n",
    "    print(\"File Export Done.\")\n",
    "\n",
    "def getDeviceCredentialsByDeviceId(deviceID = 0):\n",
    "    url = config['server']+'/device/'+deviceID+'/credentials'\n",
    "    resp = requests.get(url,headers=TBheaders)\n",
    "    responseList = resp.json()\n",
    "    #pprint(responseList)\n",
    "    return responseList['credentialsID']\n",
    "\n",
    "def getDeviceServerAttributes(deviceID = 0):\n",
    "    if deviceID == 0:\n",
    "        while(deviceID == 0):\n",
    "            try:\n",
    "                deviceID = input(\"Enter device ID: \")\n",
    "            except:\n",
    "                print(\"Invalid DeviceID\")\n",
    "    url = config['server']+'/plugins/telemetry/DEVICE/'+deviceID+'/values/attributes'\n",
    "    #pprint(url)\n",
    "    #pprint(TBheaders)\n",
    "    xresp = requests.get(url,headers=TBheaders)\n",
    "    #pprint(xresp)\n",
    "    #pprint(resp.content())\n",
    "    #print(xresp.text())\n",
    "    responseList = xresp.json()\n",
    "    #pprint(responseList)\n",
    "    #return responseList['credentialsID']\n",
    "\n",
    "\n",
    "# ** getting token for the request                                                                                         **\n",
    "print(\"Server: \",config['server'])\n",
    "token = login(config['server'], config['username'], config['password']);\n",
    "print(f\"Token: {token}\")\n",
    "TBheaders={ 'Accept': '*/*', 'X-Authorization': f\"Bearer {token}\" }\n",
    "\n",
    "\n",
    "\n",
    "# Create a datetime object representing the local date and time\n",
    "# Year, Month, Day, Hour, Minute\n",
    "today_dt = datetime.datetime.now()\n",
    "start = datetime.datetime.now()+ relativedelta(months=-2)\n",
    "\n",
    "start_dt = datetime.datetime(start.year, start.month, start.day, 18, 0)\n",
    "end_dt = datetime.datetime(today_dt.year, today_dt.month, today_dt.day, 6, 00)\n",
    "print (start_dt, end_dt)\n",
    "\n",
    "# Convert to a specific time zone (e.g., UTC)\n",
    "start_tz_utc = pytz.timezone(\"UTC\")\n",
    "start_dt_utc = start_tz_utc.localize(start_dt)\n",
    "end_tz_utc = pytz.timezone(\"UTC\")\n",
    "end_dt_utc = end_tz_utc.localize(end_dt)\n",
    "\n",
    "# Extract the Unix timestamp\n",
    "startTS = int(start_dt_utc.timestamp())*1000\n",
    "endTS = int(end_dt_utc.timestamp())*1000\n",
    "\n",
    "# Use for relative time frames\n",
    "#startTS = int((datetime.now() - timedelta(days=30)  - datetime(1970, 1, 1)).total_seconds() * 1000) # 30 days ago\n",
    "#endTS = int((datetime.datetime.utcnow() - datetime.datetime(1970, 1, 1)).total_seconds() * 1000) # now\n",
    "\n",
    "# print(startTS, endTS)\n",
    "\n",
    "\n",
    "\n",
    "# ** customer ID for the request                                                                                            **\n",
    "# getCustomerDevices(custID, textSearch=None):\n",
    "# 7576b020-ecae-11ec-b72b-5dd76ca52a2b = Cherkhauer Customer ID\n",
    "# ABE-DRAGINO-GROPOINT-CHERKHAUER = Devices with names beginning with \"ABE-DRAGINO-GROPOINT-CHERKHAUER\"\n",
    "devices = getCustomerDevices(\"7576b020-ecae-11ec-b72b-5dd76ca52a2b\",\"ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE\")\n",
    "# pprint(devices)\n",
    "\n",
    "totalLength = 0\n",
    "# keys to retrieve\n",
    "#keys = [\"data_TempC_SHT\",\"data_Hum_SHT\"]\n",
    "#keys = [\"data_ambient_temperature\",\"data_input1_frequency\",\"data_input1_frequency_to_moisture\",\"data_Input2_voltage\",\"data_Input2_voltage_to_temp\",\"data_light_intensity\",\"data_relative_humidity\"]\n",
    "keys = [\"data_soil_moisture1\",\"data_soil_moisture2\",\"data_soil_moisture3\",\"data_soil_moisture4\",\"data_tem1\",\"data_tem2\",\"data_tem3\",\"data_tem4\",\"data_tem5\",\"data_tem6\",\"data_tem7\"]\n",
    "\n",
    "outputCSV(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa92aa9",
   "metadata": {},
   "source": [
    "# Initial data reading process\n",
    "***\n",
    "## readraw_data Function\n",
    "\n",
    "The `readraw_data` function is used to read raw data from a CSV file and parse it into a pandas DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `destination`: The path where the output file will be saved.\n",
    "- `filename`: The name of the CSV file to be read.\n",
    "- `foutname`: The name of the output file.\n",
    "- `st_date`: The start date for the data to be read.\n",
    "- `ed_date`: The end date for the data to be read.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `raw_data`: A pandas DataFrame that contains the data read from the CSV file.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by using the pandas `read_csv` function to read the CSV file. It specifies the delimiter as ';' and parses the 'Timestamp' column as dates. It also specifies the data types for the soil moisture columns to be float64.\n",
    "\n",
    "The function then returns the DataFrame.\n",
    "***\n",
    "## station_data_clean Function\n",
    "\n",
    "The `station_data_clean` function is used to clean the data for a specific station.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `raw_data`: A pandas DataFrame that contains the raw data to be cleaned.\n",
    "- `station`: An integer that represents the station ID.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `station_data`: A pandas DataFrame that contains the cleaned data for the specified station.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by filtering the raw data for the specified station. It then performs any necessary cleaning operations, such as removing missing values, outliers, or incorrect data.\n",
    "\n",
    "The function then returns the cleaned data for the specified station.\n",
    "\n",
    "Please note that the actual code for the `station_data_clean` function is not provided, so the parameters and functionality are assumed based on typical usage. If you provide the actual code of the `station_data_clean` function, I can give a more accurate explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13944bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is ::  2024-05-16\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import statistics as stats\n",
    "import os\n",
    "\n",
    "\n",
    "today = dt.date.today()\n",
    "# today = datetime(2009, 7, 6, 0, 0)\n",
    "strtoday = today.strftime(\"%Y%m%d\")\n",
    "print ('Today is :: ',today)\n",
    "\n",
    "\n",
    "\n",
    "def readraw_data(destination, filename, foutname, st_date, ed_date):\n",
    "    print ('File name is ::',filename)\n",
    "    # open the file\n",
    "    # TODO delimeter should be changed to ',' for the csv file.  ts <=> Timestamp.  delimiter=',' <=> delimiter=';'\n",
    "    raw_data = pd.read_csv(filename,delimiter=',', parse_dates=['ts'],\n",
    "                          dtype={'data_soil_moisture1':np.float64,\n",
    "                                 'data_soil_moisture2':np.float64,\n",
    "                                 'data_soil_moisture3':np.float64,\n",
    "                                 'data_soil_moisture4':np.float64},\n",
    "                          na_values=['Invalid data']\n",
    "                          )\n",
    "    raw_columns = raw_data.columns.tolist()\n",
    "    # raw_data['Timestamp'] = pd.to_datetime(raw_data['Timestamp'])\n",
    "    # drop the temperature data from the list\n",
    "    for i in range(len(raw_columns)):\n",
    "        if 'tem' in raw_columns[i] or 'dev' in raw_columns[i]:\n",
    "            raw_data = raw_data.drop(columns=raw_columns[i])\n",
    "            \n",
    "    raw_data.set_index(['ts'])\n",
    "    print(raw_data.info())\n",
    "    \n",
    "    # change the name of the Entity Name column\n",
    "    raw_data['Entity Name'] = raw_data['Entity Name'].str.replace('ABE-DRAGINO-GROPOINT-CHERKHAUER-ACRE-','')\n",
    "    stationlist = sorted(raw_data['Entity Name'].unique())\n",
    "        \n",
    "    \n",
    "    # after checking the null values, filter the data\n",
    "    clean_df = pd.DataFrame(columns = ['Station','Layer1', 'Layer2', 'Layer3', 'Layer4'])\n",
    "    for station in stationlist:\n",
    "        # clean the data according to the cleaning procedure\n",
    "        sample_df = station_data_clean(destination, raw_data, station)\n",
    "        \n",
    "        clean_df = pd.concat([clean_df,sample_df])\n",
    "        \n",
    "    clean_df.to_csv(destination+foutname,index=True)\n",
    "    \n",
    "    # convert headers only with numbers\n",
    "    raw_columns = raw_data.columns.tolist()\n",
    "    \n",
    "    # group the dataframe and turn them into another dataframe with 'Entity Name' as columns.\n",
    "    # this step should be done by layer.\n",
    "    \n",
    "\n",
    "    return raw_data, raw_columns, clean_df, stationlist, raw_columns\n",
    "\n",
    "\n",
    "\n",
    "# ********************************************************************************************************************\n",
    "# * this is the function that goes into readraw_data function.                                                       *\n",
    "# ********************************************************************************************************************\n",
    "def station_data_clean(destination, raw_data, station):\n",
    "    global start_date, end_date\n",
    "    '''\n",
    "    This is cleaning for station data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_data : dataframe\n",
    "        dataframe of the raw_data, 'raw_data' in this script\n",
    "    station : string\n",
    "        this is number of station. 4 digit number filled with zero from left\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_df : dataframe\n",
    "        this cleaned data after process\n",
    "\n",
    "    '''\n",
    "    sample_df = raw_data[raw_data['Entity Name']==station]\n",
    "    sample_df = sample_df.drop(['Entity Name'], axis=1)\n",
    "    sample_df = sample_df.set_index('ts')\n",
    "    sample_df = sample_df.set_axis(['Layer1', 'Layer2', 'Layer3', 'Layer4'], axis=1)\n",
    "    \n",
    "    # clean the data by time of interest\n",
    "    # datetime range should start by 18:00 // end by 6:00 for better analysis for everyday\n",
    "    start_date = dt.datetime(st_date[0], st_date[1], st_date[2], 0, 0, 0)\n",
    "    end_date = dt.datetime(ed_date[0], ed_date[1], ed_date[2], 6, 0, 0)\n",
    "    sample_df = sample_df[(sample_df.index > start_date) & (sample_df.index < end_date)]\n",
    "    \n",
    "    # according to the number of the data length,\n",
    "    # if it is more than 1, the dataframe will be made\n",
    "    # if it is 0, below process will be skipped\n",
    "    totnum = len(sample_df)\n",
    "    if totnum > 0:    \n",
    "        print ('\\n\\n')\n",
    "        txt = ' raw_data info for station '+station+' '\n",
    "        print(txt.center(60,'='),end='\\n')\n",
    "        \n",
    "        # if value is null value from beginning\n",
    "        print(' NaN values info '.center(60,':'))\n",
    "        na_df = sample_df[sample_df.isna().any(axis=1)]\n",
    "        sample_df = sample_df.dropna()\n",
    "        print('NaN values are ::',len(na_df),'out of',totnum,'\\nerror rate:',round(len(na_df)/totnum*100,2),'%',end='\\n\\n')\n",
    "        \n",
    "        # if value is out of range\n",
    "        sample_df[(sample_df>=100.0) | (sample_df<=0.0)] = np.nan\n",
    "        outrange_df = sample_df[sample_df.isna().any(axis=1)]\n",
    "        sample_df = sample_df.dropna()\n",
    "        print('Out of range values are ::',len(outrange_df),'out of',totnum,'\\nerror rate:',round(len(outrange_df)/totnum*100,2),'%',end='\\n\\n')\n",
    "        \n",
    "        print(' Data Describe '.center(60,':'),end='\\n\\n')\n",
    "        print(sample_df.dtypes, end='\\n\\n')\n",
    "        print(sample_df.describe())\n",
    "        \n",
    "        # resampling in 30 min frequency\n",
    "        sample_df = sample_df.resample('30min').mean()\n",
    "        \n",
    "        # make boxplot per layer\n",
    "        boxplot = sample_df.boxplot(column=['Layer1', 'Layer2', 'Layer3'],figsize=(8,4), ylabel='Soil Moisture (%)')\n",
    "        plt.title(int(station))\n",
    "        plt.savefig(destination+'/Soil_moisture_graphs/'+strtoday+'_'+station+'.png',dpi=600)\n",
    "        # plt.show()\n",
    "        \n",
    "        # add station code back\n",
    "        sample_df['Station'] = int(station)\n",
    "    \n",
    "        print('='*60)\n",
    "        \n",
    "    else:\n",
    "        txt = station+' '\n",
    "        print(txt.center(60,'='),end='\\n\\n')\n",
    "    '''\n",
    "    # save station dataframe as csv file == turned off for cal_deficit\n",
    "    try:\n",
    "        sample_df.to_csv(destination+'station_data/'+station+'_cleaned.csv',sep=',')\n",
    "        print('Transmitter data exported.')\n",
    "    except:\n",
    "        print('Transmitter data export failed.')\n",
    "    '''\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d74da",
   "metadata": {},
   "source": [
    "## Rainfinder Function\n",
    "\n",
    "The `rainfinder` function is used to identify significant rain events in a given dataset. The function is designed to analyze weather data and detect periods of rainfall based on certain conditions or thresholds.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `data`: A pandas DataFrame that contains the weather data to be analyzed. The DataFrame should be indexed by 'Timestamp and Station'.\n",
    "- `station`: An integer that represents the station ID.\n",
    "- `header`: A list that contains the headers of the data.\n",
    "- `threshold_moist`: An integer that represents the threshold of soil moisture difference to decide if it was a significant rain event or not.\n",
    "- `raintimestep`: An integer that represents the number of time steps to consider for the rolling window.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `bumplist2`: A list of lists. Each inner list represents a date (in the format [year, month, day]) when a significant rain event (or \"bump\") was detected.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by first filtering the data for the specified station and removing any missing values. It then calculates the difference in soil moisture between each time step and filters out the time steps where the difference is greater than the specified threshold.\n",
    "\n",
    "The function then uses a rolling window to find the minimum and maximum soil moisture values within each window. It calculates the gap between the min and max values and filters out the time steps where the gap is greater than the threshold.\n",
    "\n",
    "The function then organizes the dates of these significant rain events, removes duplicates, and sorts the list. The sorted list of dates is returned.\n",
    "\n",
    "The function uses pandas for data manipulation and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b747502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rainfinder(data, station, header, threshold_moist, raintimestep):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "        organized dataframe / index is 'Timestamp and Station'\n",
    "    stationlist : list\n",
    "        list of the stations\n",
    "    header : list\n",
    "        header of the data\n",
    "    threshold_moist : int\n",
    "        threshold of soil moisture difference to decide it was huge rain event or not\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data_dur_all : list of dataframe\n",
    "        this is dataframes with time range of interest\n",
    "        dataframe order is accordance with order of station list (stationlist)\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: remove stationlist for the final version\n",
    "    text = station +' :: '+target+'  Rainfinder'\n",
    "    print (text.center(60,':'))\n",
    "\n",
    "    data_dur = data[data['Station']==int(station)]\n",
    "    data_dur = data_dur.dropna()\n",
    "    \n",
    "        \n",
    "    # make a rolling window column from interpolation\n",
    "    # data_dur[target] = data_dur[target].interpolate()\n",
    "    print (station,'|','LEN =', len(data_dur))\n",
    "    \n",
    "    # find differene between each time step\n",
    "    temp = data_dur[target].diff()\n",
    "    filtered_temp = temp.to_frame(name=target).query('{target} > @threshold_moist'.format(target = target))\n",
    "    \n",
    "    # find biggest difference within 3 hours (6 timesteps) == bump\n",
    "    data_dur['min_'+target] = data_dur[target].rolling(window=raintimestep).min()\n",
    "    data_dur['max_'+target] = data_dur[target].rolling(window=raintimestep).max()\n",
    "    data_dur['gap_'+target] = data_dur['max_'+target] - data_dur['min_'+target]\n",
    "    filtered_temp = data_dur.query('gap_{target} > @threshold_moist'.format(target = target))\n",
    "    #print (filtered_temp)\n",
    "\n",
    "    # organizing the date of bump\n",
    "    bumplist = filtered_temp.index.to_list()        \n",
    "    for i in range(len(bumplist)):\n",
    "        # make a list of date with the bump in the form of [year, month, day]\n",
    "        bumplist[i] = [bumplist[i].year, bumplist[i].month, bumplist[i].day]\n",
    "    # remove duplicates in bumplist\n",
    "    bumplist2 = list(set(map(tuple, bumplist)))\n",
    "    # sort the list and convert tuples into list\n",
    "    bumplist2 = sorted(bumplist2)\n",
    "    for i in range(len(bumplist2)):\n",
    "        bumplist2[i] = list(bumplist2[i])\n",
    "\n",
    "    print ('Searched ', len(bumplist2), 'days with bump events')\n",
    "    print (bumplist2)\n",
    "\n",
    "    return bumplist2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70026a60",
   "metadata": {},
   "source": [
    "\n",
    "## FCfinder Function\n",
    "\n",
    "The `FCfinder` function is used to find the field capacity of soil after a given number of days from a specified start date. Field capacity is the amount of soil moisture or water content held in the soil after excess water has drained away and the rate of downward movement has decreased. This capacity is reached within 2–3 days after rain or irrigation in typical soil conditions.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `data`: A pandas DataFrame that contains the data to be analyzed. The DataFrame should be indexed by 'Timestamp and Station'.\n",
    "- `station`: An integer that represents the station ID.\n",
    "- `header`: A list that contains the headers of the data.\n",
    "- `start_date`: A list that contains the start date of the data search in the format [year, month, day].\n",
    "- `search_days`: An integer that represents the number of days to search for field capacity.\n",
    "- `search_range`: An integer that represents the range (in hours) for moving field capacity.\n",
    "- `search_slope`: A float that represents the slope for searching.\n",
    "- `threshold_hour`: An integer that represents the threshold hours that remains flat soil moisture behavior to confirm it is a field capacity point.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `data_dur_all`: A list of pandas DataFrames. Each DataFrame contains a time range of interest. The order of the DataFrames corresponds to the order of the station list.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "The function works by iterating over a specified number of days from the start date. For each day, it identifies a time range around midnight and checks if the standard deviation of the soil moisture within this time range is less than the specified slope. If it is, the function considers this as a constant moisture behavior and records the date and the mean soil moisture value. The function then plots the soil moisture and the field capacity points, saves the plot as a PNG file, and writes the field capacity records to a CSV file.\n",
    "\n",
    "The function uses a rolling window to smooth the soil moisture data and uses the standard deviation to identify constant moisture behavior. It also uses matplotlib for plotting and os for file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed02d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCfinder(data, station, header, start_date, search_days, search_range, search_slope, threshold_hour):\n",
    "    global searched, precip_search1, precip_search2, precip_delta, precip_max\n",
    "\n",
    "    # print ('Station',station, 'Date',start_date)\n",
    "    # important == rl means rolling window. if you don't want, remove it.\n",
    "    search_target = target\n",
    "    # search_target = 'rl_'+target\n",
    "    layer_index = int(target[-1])-1\n",
    "\n",
    "    data_dur_all = [] # this is test list to save all the data_dur searched\n",
    "    data_dur = data[data['Station']==int(station)]\n",
    "    \n",
    "        \n",
    "    # make a rolling window column from interpolation\n",
    "    data_dur['Layer1'] = data_dur['Layer1'].interpolate()\n",
    "    data_dur['Layer2'] = data_dur['Layer2'].interpolate()\n",
    "    data_dur['Layer3'] = data_dur['Layer3'].interpolate()\n",
    "    data_dur['Layer4'] = data_dur['Layer4'].interpolate()\n",
    "    # data_dur['rl_'+target] = data_dur[target].rolling(window=3).mean()\n",
    "    \n",
    "\n",
    "    '''\n",
    "    From here, we need to find out point where soil moisture value is consistent for 2 hours (threshold_hour)\n",
    "    And consistent variation for this hour is < 1% (search_slope)\n",
    "    Searching range will be +- 4hrs from midnight (search_range)\n",
    "    '''\n",
    "    # set the database only for the search_target\n",
    "    target_data = data_dur[search_target]\n",
    "    \n",
    "    # find 00:00 hour of everyday within TOI (7 days)\n",
    "    dayrange = search_days\n",
    "    \n",
    "    # this list is for the searched fc dates\n",
    "    searched_list = []\n",
    "    \n",
    "    # searching field capacity (daily)\n",
    "    for days in range(1,dayrange+1):\n",
    "        # Going to find the field capacity after given days from the start_date\n",
    "        st_datetime = dt.datetime(start_date[0], start_date[1], start_date[2], 6, 0, 0)\n",
    "\n",
    "        # stamp2 is 4 hours before the midnight of the stamp1\n",
    "        # stamp3 is 4 hours after the midnight of the stamp2\n",
    "        # add 'days' to search every daily step\n",
    "        stamp1 = st_datetime + dt.timedelta(days=days)\n",
    "        stamp1 = stamp1.replace(hour=0, minute=0, second=0)\n",
    "        \n",
    "        # step 1. select the time range of interest\n",
    "        # once you find the time, you will search there is constant soil moisture behavior or not\n",
    "        # set the searching time\n",
    "        stamp2, stamp3 = stamp1 - dt.timedelta(hours=search_range), stamp1 + dt.timedelta(hours=search_range)\n",
    "        # stamp3 = stamp1 + dt.timedelta(hours=search_range)\n",
    "        \n",
    "        # step 2. set the searching range\n",
    "        searched = target_data[(target_data.index > stamp2) & (target_data.index < stamp3)]\n",
    "        # searched = data_search\n",
    "            \n",
    "        try:\n",
    "            # if standard deviation does not exceeds 'search_slope',\n",
    "            # this will be regarded as constant moisture behavior\n",
    "            if stats.stdev(searched) < search_slope:\n",
    "                searched_list.append([stamp1, stats.mean(searched)])\n",
    "                FC_searched = True\n",
    "            else:\n",
    "                FC_searched = False # if there is no constant behavior, FC_searched will be False == no field capacity\n",
    "        except:\n",
    "            FC_searched = False # this case is no data in the range, so FC_searched will be False == no field capacity\n",
    "        \n",
    "        if FC_searched == True:\n",
    "            print ('Field Capacity is found for',str(start_date[0])+'-'+str(start_date[1])+'-'+str(start_date[2]),':',searched_list[0][0].strftime('%Y-%m-%d'),round(searched_list[0][1],3))\n",
    "\n",
    "        \n",
    "            # list of searched field capacity this value will be only one pair.\n",
    "            searched_dates = [i[0] for i in searched_list]\n",
    "            searched_values = [i[1] for i in searched_list]\n",
    "            \n",
    "            # if field capacity is found, draw graph\n",
    "            if len(searched_dates) > 0:\n",
    "                # draw overlaying graph for the soil moisture and field capacity\n",
    "                # soil moisture is blue line and field capacity is red dot            \n",
    "                df_subset = data_dur[(data_dur.index < stamp1+dt.timedelta(days=1)) & (data_dur.index > st_datetime)]\n",
    "                # print(df_subset)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(12, 5))\n",
    "                # ax = data_dur.plot(y=target, label = 'Observed values', figsize=(15,5))\n",
    "                ax = plt.plot(df_subset.index, df_subset[target], label = 'Observed values')\n",
    "\n",
    "                # ax.set_xlim(stamp2, stamp3)\n",
    "                plt.scatter(x=searched_dates, y=searched_values, label = 'Moving FC points', marker='s', c='r', s=100)\n",
    "                plt.legend()\n",
    "                plt.gcf().autofmt_xdate()\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Soil Moisture (%)')\n",
    "                plt.title('Field Capacity for '+str(station)+' after '+st_datetime.strftime('%Y-%m-%d'))\n",
    "\n",
    "                # save graph for the soil moisture and field capacity\n",
    "                graph_dest = os.getcwd()+'/fc_graphs/'+st_datetime.strftime('%Y-%m-%d')\n",
    "                if not os.path.exists(graph_dest):\n",
    "                    os.makedirs(graph_dest)\n",
    "                plt.savefig(graph_dest+'/'+str(station)+'_'+stamp1.strftime('%Y-%m-%d')+'.png',dpi=600)\n",
    "                # plt.show()\n",
    "                print('Graph is saved.')\n",
    "            \n",
    "            # save field capacity records in csv form\n",
    "            data_dur_all.append(data_dur)\n",
    "            \n",
    "            \n",
    "            # save field capacity records in csv form\n",
    "            # print('searched_list ::',searched_list)\n",
    "            with  open(os.getcwd()+'/fc_results/'+strtoday+'_fc_record.csv', 'a+') as f:\n",
    "                for item in searched_list:\n",
    "                    strdate = item[0].strftime('%Y-%m-%d')\n",
    "                    sm_list = data_dur[data_dur.index==item[0]].iloc[0,:].values.tolist()[1:]\n",
    "                    sm_list = [str(round(i,3)) for i in sm_list]\n",
    "                    sm_list_str = ','.join(map(str, sm_list))\n",
    "                    f.write(str(station)+','+strdate+','+sm_list_str+'\\n')\n",
    "            \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30946eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fc():\n",
    "    filename = os.getcwd()+'/fc_results/'+strtoday+'_fc_record.csv'\n",
    "    # open the daily fc file & read\n",
    "    raw_data = pd.read_csv(filename,delimiter=',', parse_dates=['Date'],\n",
    "                          dtype={'data_soil_moisture1':np.float64,\n",
    "                                 'data_soil_moisture2':np.float64,\n",
    "                                 'data_soil_moisture3':np.float64,\n",
    "                                 'data_soil_moisture4':np.float64},\n",
    "                          na_values=['Invalid data']\n",
    "                          )\n",
    "    raw_data = raw_data.dropna()    # drop the rows if there is NaN values\n",
    "    raw_data.drop_duplicates(inplace=True)  # remove duplicate rows\n",
    "    raw_data.to_csv(filename,index=False)   # export the data to csv file    \n",
    "    raw_data = raw_data.sort_values(by=['Station','Date'])  # sort the data by station and date\n",
    "    last_date = raw_data.groupby('Station').tail(1) # get the last date of the data\n",
    "\n",
    "    # open the 00_Current_FC.csv file & read\n",
    "    fc_data = pd.read_csv(os.getcwd()+'/00_Current_FieldCapacity.csv',delimiter=',', parse_dates=['Date'],\n",
    "                          dtype={'data_soil_moisture1':np.float64,\n",
    "                                 'data_soil_moisture2':np.float64,\n",
    "                                 'data_soil_moisture3':np.float64,\n",
    "                                 'data_soil_moisture4':np.float64},\n",
    "                          na_values=['Invalid data']\n",
    "                          )\n",
    "    fc_data = pd.concat([fc_data,raw_data]) # concat fc_data and raw_data\n",
    "    fc_data = fc_data.drop_duplicates() # remove duplicate rows\n",
    "    fc_data = fc_data.sort_values(by=['Station','Date']) # sort the data by station and date\n",
    "    fc_data = fc_data.groupby('Station').tail(1) # get the last date of the data and update the data\n",
    "    fc_data.to_csv(os.getcwd()+'/00_Current_FieldCapacity.csv',index=False) # export the data to csv file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5e281",
   "metadata": {},
   "source": [
    "# About the 'Defecit Calc' function\n",
    "## About\n",
    "***\n",
    "\n",
    "\n",
    "The provided Python code defines two functions: `deficit_calc` and `deficit_equation`.\n",
    "\n",
    "1. `deficit_calc(thedate, rootdpth)`: This function calculates the soil moisture deficit for a given date and root depth across multiple stations. \n",
    "\n",
    "   - **Parameters**: \n",
    "     - `thedate`: The date of interest.\n",
    "     - `rootdpth`: The root depth.\n",
    "\n",
    "   - **Process**: \n",
    "     - It loops over a list of stations (`stationlist`), which is not defined in the provided code.\n",
    "     - For each station, it filters the `raw_data` DataFrame (also not defined in the provided code) to get data for that station and drops any rows with missing values.\n",
    "     - It then tries to find the soil moisture value (`sm_val`) for the date of interest and the field capacity value (`fc_val`) for the station from a CSV file.\n",
    "     - It calls the `deficit_equation` function to calculate the deficit and prints the result.\n",
    "     - If there's no data for a station, it prints a message indicating this.\n",
    "\n",
    "2. `deficit_equation(ts, station, rootdpth, sm_val, fc_val)`: This function calculates the soil moisture deficit for a given station on a specific date. \n",
    "\n",
    "   - **Parameters**: \n",
    "     - `ts`: The date.\n",
    "     - `station`: The station.\n",
    "     - `rootdpth`: The root depth.\n",
    "     - `sm_val`: The soil moisture value.\n",
    "     - `fc_val`: The field capacity value.\n",
    "\n",
    "   - **Process**: \n",
    "     - It calculates the deficit based on the root depth and the difference between the field capacity and the soil moisture value for different soil layers. The calculation is done in centimeters (hence the multiplication by 0.01).\n",
    "     - It rounds the deficit to two decimal places and returns it.\n",
    "\n",
    "The commented-out code at the top appears to be an earlier version of the deficit calculation, which is now performed by the `deficit_equation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef1f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#! Laura's equation for the defecit calculation\n",
    "df2.loc[df2['soil_moisture1'] < fcdf.fc1, 'd1']=fcdf.fc1-df2['soil_moisture1']\n",
    "df2.loc[df2['soil_moisture2'] < fcdf.fc2, 'd2']=fcdf.fc2-df2['soil_moisture2']\n",
    "df2.loc[df2['soil_moisture3'] < fcdf.fc3, 'd3']=fcdf.fc3-df2['soil_moisture3']\n",
    "df2.loc[df2['soil_moisture4'] < fcdf.fc4, 'd4']=fcdf.fc4-df2['soil_moisture4']\n",
    "\n",
    "if root < 6:\n",
    "    df2['Deficit'] = (df2['d1'])*root\n",
    "elif root < 12:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*(root-6)\n",
    "elif root < 18:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*6 + (df2['d3'])*(root-12)\n",
    "else:\n",
    "    df2['Deficit'] = (df2['d1'])*6 + (df2['d2'])*6 + (df2['d3'])*6 + (df2['d4'])*(root-18)\n",
    "\n",
    "'''\n",
    "def deficit_calc (thedate, rootdpth):\n",
    "    \n",
    "    # read the soil moisture values from raw_data close to the midnight\n",
    "    # date of the interest is ts\n",
    "    # ts = dt.date.today()\n",
    "    ts = thedate\n",
    "    # ts = pd.to_datetime(today)\n",
    "    print('Date of the interest is ',ts)\n",
    "\n",
    "    # open the file\n",
    "    f = open(os.getcwd()+'/Deficit_results/'+strtoday+'_deficit_report.csv','w') # create the file\n",
    "    f.write('Date,Station,Deficit,sm_val1,sm_val2,sm_val3,sm_val4,fc_date,fc_val1,fc_val2,fc_val3,fc_val4\\n') # write the header\n",
    "\n",
    "    for station in stationlist:\n",
    "\n",
    "        # fliter the raw_data by station and drop the NaN values\n",
    "        sample_df = raw_data[raw_data['Entity Name']==station].dropna()\n",
    "        sample_df.set_index('ts', inplace=True)\n",
    "        \n",
    "        try:\n",
    "            # get the midnight value of the date of the interest\n",
    "            iloc_idx = sample_df.index.get_indexer([ts], method='nearest')  # returns absolute index into df e.g. array([5])\n",
    "            loc_idx = sample_df.index[iloc_idx]                             # if you want named index\n",
    "            sm_val = sample_df.iloc[iloc_idx]\n",
    "            sm_val = sample_df.loc[loc_idx]                                 # as above so below...    \n",
    "            \n",
    "            # convert my_val to list\n",
    "            sm_val = sm_val.values.tolist()[0]\n",
    "            print('sm_val',sm_val)\n",
    "\n",
    "            # get the field capacity value from the 00_Current_FieldCapacity.csv\n",
    "            fcdf = pd.read_csv(os.getcwd()+'/00_Current_FieldCapacity.csv')\n",
    "            fc_val = fcdf[fcdf['Station']==int(station)].values.tolist()[0]\n",
    "            print('fc_val',fc_val)\n",
    "            \n",
    "            deficit = deficit_equation(ts,station, rootdpth, sm_val, fc_val)\n",
    "            \n",
    "            print('>>> Deficit for',ts,'at',int(station),'is',deficit,'& FC date is:',fc_val[1],end='\\n\\n')\n",
    "            f.write(str(station)+','+str(ts)+','+str(deficit)+','+','.join(str(x) for x in sm_val[1:])+','+str(fc_val[1])+','+','.join(str(x) for x in fc_val[2:])+'\\n')\n",
    "\n",
    "        \n",
    "        except:\n",
    "            txt = 'No data for  '+station\n",
    "            print(txt.center(60,'='),end='\\n\\n')\n",
    "        \n",
    "    f.close() # close the deficit report file \n",
    "\n",
    "\n",
    "\n",
    "def deficit_equation(ts,station, rootdpth, sm_val, fc_val):\n",
    "    # TODO 0-1\n",
    "    # TODO make timesseries for the deficit\n",
    "    if rootdpth < 6:\n",
    "        deficit = ((fc_val[2]-sm_val[1])*rootdpth)*0.01\n",
    "    elif rootdpth < 12:\n",
    "        deficit = ((fc_val[2]-sm_val[1])*6 + (fc_val[3]-sm_val[2])*(rootdpth-6))*0.01\n",
    "    elif rootdpth < 18:\n",
    "        deficit = ((fc_val[2]-sm_val[1])*6 + (fc_val[3]-sm_val[2])*6 + (fc_val[4]-sm_val[3])*(rootdpth-12))*0.01\n",
    "    else:\n",
    "        deficit = ((fc_val[2]-sm_val[1])*6 + (fc_val[3]-sm_val[2])*6 + (fc_val[4]-sm_val[3])*6 + (fc_val[5]-sm_val[4])*(rootdpth-18))*0.01\n",
    "    deficit = round(deficit,2)\n",
    "    return deficit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3062411",
   "metadata": {},
   "source": [
    "# Below is implementation Block\n",
    "- **Station List**: This line prints the list of stations to the console.\n",
    "\n",
    "- **Target Layer**: This is the target layer for the analysis. The variable `target` is set to 'Layer2'.\n",
    "\n",
    "- **Rainfinder Variables**: The variables `threshold_moist` and `raintimestep` are set for the `rainfinder` function. `threshold_moist` is the soil moisture difference threshold to decide if a significant rain event has occurred. `raintimestep` is the time step for rain detection, where 1 represents half an hour and 12 represents 6 hours.\n",
    "\n",
    "- **FCfinder Variables**: The variables `search_days`, `search_range`, `search_slope`, and `threshold_hour` are set for the `FCfinder` function. `search_days` is the number of days to search for field capacity. `search_range` is the number of hours to search around a given timestamp. `search_slope` is the standard deviation threshold for soil moisture behavior. `threshold_hour` is the number of hours with consistent soil moisture behavior.\n",
    "\n",
    "- **File Creation**: A new CSV file is created in the `fc_results` directory to store the daily field capacity results. The file is named with the current date (`strtoday`). The first line of the file is a header line with the column names.\n",
    "\n",
    "- **Main Loop**: This is the main loop of the program. For each station in the `stationlist`, it prints the station ID and target layer. It then calls the `rainfinder` function to get a list of dates (`bumplist`) where significant rain events occurred. For each of these dates, it calls the `FCfinder` function to estimate the field capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91870e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name is :: /Users/DK/Documents/GitHub/SoilMoisture_analysis/Raw_data/Calc_def_test_0725.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103509 entries, 0 to 103508\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   ts                   103509 non-null  datetime64[ns]\n",
      " 1   Entity Name          103509 non-null  object        \n",
      " 2   data_soil_moisture1  101602 non-null  float64       \n",
      " 3   data_soil_moisture2  101602 non-null  float64       \n",
      " 4   data_soil_moisture3  101602 non-null  float64       \n",
      " 5   data_soil_moisture4  101602 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(1)\n",
      "memory usage: 4.7+ MB\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0111 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 45 out of 7924 \n",
      "error rate: 0.57 %\n",
      "\n",
      "Out of range values are :: 1783 out of 7924 \n",
      "error rate: 22.5 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  6096.000000  6096.000000  6096.000000  6096.000000\n",
      "mean     25.206348    35.807448    37.175344    36.485761\n",
      "std       6.031766     5.507786     5.358744     6.496116\n",
      "min      16.600000    16.500000    16.500000    16.500000\n",
      "25%      20.700000    30.700000    31.700000    31.200000\n",
      "50%      24.100000    34.600000    37.400000    36.600000\n",
      "75%      27.900000    41.800000    41.700000    39.900000\n",
      "max      46.800000    49.900000    51.600000    54.900000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0112 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 103 out of 8446 \n",
      "error rate: 1.22 %\n",
      "\n",
      "Out of range values are :: 1799 out of 8446 \n",
      "error rate: 21.3 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  6544.000000  6544.000000  6544.000000  6544.000000\n",
      "mean     32.775076    36.897051    39.161553    38.863936\n",
      "std       5.697561     6.341650     4.726690     5.864225\n",
      "min      16.000000    17.000000    18.000000    16.500000\n",
      "25%      28.500000    29.200000    35.200000    36.475000\n",
      "50%      32.300000    39.500000    40.700000    38.000000\n",
      "75%      36.500000    42.500000    42.000000    40.600000\n",
      "max      47.400000    50.400000    52.000000    54.900000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0113 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 410 out of 8584 \n",
      "error rate: 4.78 %\n",
      "\n",
      "Out of range values are :: 2223 out of 8584 \n",
      "error rate: 25.9 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  5951.000000  5951.000000  5951.000000  5951.000000\n",
      "mean     20.598773    33.777046    34.362779    34.232280\n",
      "std       7.143112     8.497009     4.451650     5.898806\n",
      "min       8.700000    14.500000    16.000000    16.500000\n",
      "25%      15.200000    35.400000    33.300000    31.400000\n",
      "50%      23.000000    36.900000    36.200000    31.900000\n",
      "75%      25.200000    37.500000    36.600000    35.100000\n",
      "max      48.500000    49.400000    45.800000    47.900000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0114 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 311 out of 8533 \n",
      "error rate: 3.64 %\n",
      "\n",
      "Out of range values are :: 1775 out of 8533 \n",
      "error rate: 20.8 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  6447.000000  6447.000000  6447.000000  6447.000000\n",
      "mean     28.073119    35.735955    31.282286    32.193547\n",
      "std       5.463293     4.958623     4.015389     5.502055\n",
      "min      17.000000    16.000000    17.000000    16.500000\n",
      "25%      23.900000    32.200000    28.000000    29.300000\n",
      "50%      27.000000    36.400000    32.200000    31.800000\n",
      "75%      31.400000    38.900000    34.000000    33.600000\n",
      "max      49.000000    46.800000    42.400000    52.300000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0116 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 73 out of 4841 \n",
      "error rate: 1.51 %\n",
      "\n",
      "Out of range values are :: 1818 out of 4841 \n",
      "error rate: 37.55 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  2950.000000  2950.000000  2950.000000  2950.000000\n",
      "mean     31.691220    38.787322    43.031390    45.594847\n",
      "std       5.199298     8.839399     8.664924     9.226535\n",
      "min      14.000000    17.000000    17.000000    18.000000\n",
      "25%      30.200000    35.200000    44.000000    48.500000\n",
      "50%      31.400000    36.200000    44.900000    49.000000\n",
      "75%      35.000000    49.100000    48.700000    49.400000\n",
      "max      46.300000    51.600000    50.800000    49.900000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0117 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 150 out of 150 \n",
      "error rate: 100.0 %\n",
      "\n",
      "Out of range values are :: 0 out of 150 \n",
      "error rate: 0.0 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "       Layer1  Layer2  Layer3  Layer4\n",
      "count     0.0     0.0     0.0     0.0\n",
      "mean      NaN     NaN     NaN     NaN\n",
      "std       NaN     NaN     NaN     NaN\n",
      "min       NaN     NaN     NaN     NaN\n",
      "25%       NaN     NaN     NaN     NaN\n",
      "50%       NaN     NaN     NaN     NaN\n",
      "75%       NaN     NaN     NaN     NaN\n",
      "max       NaN     NaN     NaN     NaN\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0118 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 441 out of 8805 \n",
      "error rate: 5.01 %\n",
      "\n",
      "Out of range values are :: 1789 out of 8805 \n",
      "error rate: 20.32 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  6575.000000  6575.000000  6575.000000  6575.000000\n",
      "mean     20.982631    33.642753    34.111817    37.387163\n",
      "std       4.836671     5.491540     3.866929     5.569656\n",
      "min      13.500000    15.000000    15.500000    17.000000\n",
      "25%      16.500000    29.300000    32.700000    34.500000\n",
      "50%      19.700000    34.000000    34.300000    36.100000\n",
      "75%      24.300000    38.500000    35.400000    41.300000\n",
      "max      45.400000    49.100000    51.100000    53.800000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0554 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 2610 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 0 out of 2610 \n",
      "error rate: 0.0 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  2610.000000  2610.000000  2610.000000  2610.000000\n",
      "mean     36.077893    42.600077    46.006858    46.568774\n",
      "std       4.310694     4.018416     2.155050     0.546646\n",
      "min      31.000000    36.100000    42.800000    45.400000\n",
      "25%      33.300000    39.100000    44.400000    46.100000\n",
      "50%      34.400000    41.800000    45.700000    46.600000\n",
      "75%      38.700000    45.200000    47.400000    47.000000\n",
      "max      48.700000    52.000000    50.900000    48.000000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0555 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 3379 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 0 out of 3379 \n",
      "error rate: 0.0 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  3379.000000  3379.000000  3379.000000  3379.000000\n",
      "mean     41.513229    45.735691    45.576620    47.791181\n",
      "std       3.832228     3.418393     4.140782     4.394230\n",
      "min      32.500000    38.500000    37.500000    41.300000\n",
      "25%      39.850000    43.600000    42.900000    44.100000\n",
      "50%      41.000000    45.700000    45.000000    47.700000\n",
      "75%      43.200000    47.900000    48.400000    50.200000\n",
      "max      61.000000    59.500000    58.300000    61.700000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0556 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 3219 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 2 out of 3219 \n",
      "error rate: 0.06 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  3217.000000  3217.000000  3217.000000  3217.000000\n",
      "mean     17.033851    21.636556    27.599161    38.782188\n",
      "std       4.747048     6.449718     7.496353     6.250702\n",
      "min       9.700000    15.000000    19.700000    31.700000\n",
      "25%      13.700000    17.200000    22.800000    33.500000\n",
      "50%      15.300000    18.100000    23.600000    35.900000\n",
      "75%      21.100000    27.300000    33.300000    44.500000\n",
      "max      46.900000    50.800000    52.700000    53.200000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0557 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 2636 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 0 out of 2636 \n",
      "error rate: 0.0 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  2636.000000  2636.000000  2636.000000  2636.000000\n",
      "mean     26.470068    32.665630    36.303907    37.617527\n",
      "std       5.967811     3.774344     3.751078     3.469429\n",
      "min      12.700000    21.000000    21.300000    22.700000\n",
      "25%      22.800000    30.900000    34.500000    35.500000\n",
      "50%      24.700000    33.100000    35.800000    37.800000\n",
      "75%      29.300000    34.800000    36.800000    39.200000\n",
      "max      43.600000    44.600000    46.400000    45.100000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 0558 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 2622 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 2 out of 2622 \n",
      "error rate: 0.08 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  2620.000000  2620.000000  2620.000000  2620.000000\n",
      "mean     30.058931    36.739504    33.585763    29.185382\n",
      "std       8.016491     5.223518     4.331011     6.408250\n",
      "min       2.600000    21.000000    17.200000    15.000000\n",
      "25%      25.500000    33.700000    34.300000    20.800000\n",
      "50%      31.700000    37.700000    34.500000    32.100000\n",
      "75%      35.700000    39.300000    34.800000    34.000000\n",
      "max      42.200000    45.700000    37.700000    42.600000\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============== raw_data info for station 2492 ==============\n",
      "::::::::::::::::::::: NaN values info ::::::::::::::::::::::\n",
      "NaN values are :: 0 out of 3055 \n",
      "error rate: 0.0 %\n",
      "\n",
      "Out of range values are :: 901 out of 3055 \n",
      "error rate: 29.49 %\n",
      "\n",
      ":::::::::::::::::::::: Data Describe :::::::::::::::::::::::\n",
      "\n",
      "Layer1    float64\n",
      "Layer2    float64\n",
      "Layer3    float64\n",
      "Layer4    float64\n",
      "dtype: object\n",
      "\n",
      "            Layer1       Layer2       Layer3       Layer4\n",
      "count  2154.000000  2154.000000  2154.000000  2154.000000\n",
      "mean     19.253296    37.993593    32.764345    28.757939\n",
      "std       3.379296     2.125314     4.238324     2.865595\n",
      "min      15.200000    35.300000    28.000000    25.000000\n",
      "25%      17.300000    36.400000    29.000000    25.500000\n",
      "50%      18.400000    36.800000    31.100000    29.150000\n",
      "75%      20.300000    40.200000    38.200000    31.600000\n",
      "max      37.500000    45.100000    39.900000    32.600000\n",
      "============================================================\n",
      "Station List :: ['0111', '0112', '0113', '0114', '0116', '0117', '0118', '0554', '0555', '0556', '0557', '0558', '2492']\n",
      "\n",
      "\n",
      "\n",
      "=======================0111 :: Layer2=======================\n",
      ":::::::::::::::::0111 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0111 | LEN = 2372\n",
      "Searched  2 days with bump events\n",
      "[[2023, 7, 8], [2023, 7, 15]]\n",
      "Field Capacity is found for 2023-7-8 : 2023-07-17 42.582\n",
      "Graph is saved.\n",
      "Field Capacity is found for 2023-7-15 : 2023-07-17 42.582\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0112 :: Layer2=======================\n",
      ":::::::::::::::::0112 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0112 | LEN = 2431\n",
      "Searched  1 days with bump events\n",
      "[[2023, 6, 12]]\n",
      "Field Capacity is found for 2023-6-12 : 2023-06-22 29.39\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0113 :: Layer2=======================\n",
      ":::::::::::::::::0113 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0113 | LEN = 2270\n",
      "Searched  2 days with bump events\n",
      "[[2023, 6, 15], [2023, 6, 17]]\n",
      "\n",
      "\n",
      "\n",
      "=======================0114 :: Layer2=======================\n",
      ":::::::::::::::::0114 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0114 | LEN = 2415\n",
      "Searched  1 days with bump events\n",
      "[[2023, 5, 26]]\n",
      "Field Capacity is found for 2023-5-26 : 2023-05-31 35.913\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0116 :: Layer2=======================\n",
      ":::::::::::::::::0116 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0116 | LEN = 1024\n",
      "Searched  10 days with bump events\n",
      "[[2023, 5, 22], [2023, 5, 23], [2023, 5, 24], [2023, 5, 25], [2023, 5, 26], [2023, 5, 27], [2023, 5, 28], [2023, 5, 29], [2023, 5, 30], [2023, 6, 12]]\n",
      "Field Capacity is found for 2023-6-12 : 2023-06-23 36.539\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0117 :: Layer2=======================\n",
      ":::::::::::::::::0117 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0117 | LEN = 0\n",
      "Searched  0 days with bump events\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======================0118 :: Layer2=======================\n",
      ":::::::::::::::::0118 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0118 | LEN = 2449\n",
      "Searched  1 days with bump events\n",
      "[[2023, 6, 25]]\n",
      "Field Capacity is found for 2023-6-25 : 2023-06-27 29.536\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0554 :: Layer2=======================\n",
      ":::::::::::::::::0554 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0554 | LEN = 1226\n",
      "Searched  0 days with bump events\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======================0555 :: Layer2=======================\n",
      ":::::::::::::::::0555 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0555 | LEN = 1307\n",
      "Searched  0 days with bump events\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======================0556 :: Layer2=======================\n",
      ":::::::::::::::::0556 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0556 | LEN = 1250\n",
      "Searched  4 days with bump events\n",
      "[[2023, 7, 8], [2023, 7, 15], [2023, 7, 19], [2023, 7, 23]]\n",
      "Field Capacity is found for 2023-7-8 : 2023-07-10 18.261\n",
      "Graph is saved.\n",
      "Field Capacity is found for 2023-7-15 : 2023-07-18 23.065\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0557 :: Layer2=======================\n",
      ":::::::::::::::::0557 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0557 | LEN = 970\n",
      "Searched  2 days with bump events\n",
      "[[2023, 7, 5], [2023, 7, 6]]\n",
      "Field Capacity is found for 2023-7-5 : 2023-07-17 35.197\n",
      "Graph is saved.\n",
      "Field Capacity is found for 2023-7-6 : 2023-07-17 35.197\n",
      "Graph is saved.\n",
      "\n",
      "\n",
      "\n",
      "=======================0558 :: Layer2=======================\n",
      ":::::::::::::::::0558 :: Layer2  Rainfinder:::::::::::::::::\n",
      "0558 | LEN = 971\n",
      "Searched  0 days with bump events\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======================2492 :: Layer2=======================\n",
      ":::::::::::::::::2492 :: Layer2  Rainfinder:::::::::::::::::\n",
      "2492 | LEN = 1284\n",
      "Searched  0 days with bump events\n",
      "[]\n",
      "Field Capacity Update is Done.\n",
      "\n",
      "\n",
      "\n",
      "Date of the interest is  2023-07-12 00:00:00\n",
      "sm_val ['0111', 25.0, 35.6, 37.9, 33.4]\n",
      "fc_val [111, '2023-07-17', 36.767, 42.6, 45.1, 49.733]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 111 is 2.54 & FC date is: 2023-07-17\n",
      "\n",
      "sm_val ['0112', 30.8, 39.7, 41.2, 38.5]\n",
      "fc_val [112, '2024-04-12', 38.0, 46.4, 48.3, 47.1]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 112 is 1.78 & FC date is: 2024-04-12\n",
      "\n",
      "sm_val ['0113', 24.4, 37.3, 35.9, 31.3]\n",
      "=====================No data for  0113======================\n",
      "\n",
      "sm_val ['0114', 29.9, 38.6, 34.8, 33.8]\n",
      "fc_val [114, '2023-05-31', 21.151, 35.913, 33.739, 32.135]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 114 is -0.85 & FC date is: 2023-05-31\n",
      "\n",
      "=====================No data for  0116======================\n",
      "\n",
      "=====================No data for  0117======================\n",
      "\n",
      "sm_val ['0118', 23.2, 39.1, 34.4, 35.3]\n",
      "fc_val [118, '2023-06-27', 19.3, 29.6, 33.2, 35.1]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 118 is -0.89 & FC date is: 2023-06-27\n",
      "\n",
      "sm_val ['0554', 34.7, 43.4, 46.4, 46.2]\n",
      "=====================No data for  0554======================\n",
      "\n",
      "sm_val ['0555', 40.5, 45.3, 43.8, 44.0]\n",
      "=====================No data for  0555======================\n",
      "\n",
      "sm_val ['0556', 14.5, 17.9, 23.3, 33.5]\n",
      "fc_val [556, '2023-07-18', 19.267, 23.1, 20.3, 36.067]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 556 is 0.57 & FC date is: 2023-07-18\n",
      "\n",
      "sm_val ['0557', 23.0, 33.5, 34.8, 35.9]\n",
      "fc_val [557, '2023-07-17', 32.3, 35.2, 36.7, 38.767]\n",
      ">>> Deficit for 2023-07-12 00:00:00 at 557 is 0.95 & FC date is: 2023-07-17\n",
      "\n",
      "sm_val ['0558', 27.7, 39.0, 34.5, 32.7]\n",
      "=====================No data for  0558======================\n",
      "\n",
      "sm_val ['2492', 31.0, 41.7, 30.1, 25.4]\n",
      "=====================No data for  2492======================\n",
      "\n",
      "*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Deficit Calculation is Done.\n",
      "\n",
      "Check the ./fc_results/20240516_fc_record.csv folder for the today's field capacity data.\n",
      "Check the ./fc_graphs/ folder for the today's field capacity graph\n",
      "Check the ./00_Current_FieldCapacity.csv file for the most updated field capacity data.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "Implementation Block\n",
    "\n",
    "set up the variables for running the programs\n",
    "you will set the destination of the files (directory)\n",
    "you will set the date of your interest.\n",
    "usually, it is set to today's date\n",
    "\n",
    "'''\n",
    "\n",
    "destination = os.getcwd()+'/'\n",
    "today_dt = datetime.datetime.now()\n",
    "today_dt = datetime.datetime(today_dt.year, today_dt.month, today_dt.day, 6, 00)\n",
    "filename = destination + 'Raw_data/data-'+today_dt.strftime('%Y-%m-%d-%H-%M')+'.csv'\n",
    "filename = destination + 'Raw_data/Calc_def_test_0725.csv'\n",
    "# ! filename should be changed to the file name of the downloaded data\n",
    "foutname = './Clean_data/SM_data'+strtoday+'.csv'\n",
    "\n",
    "# set the time of interest and location [year, month, day]\n",
    "#  this is for the data of soil moisture. = for the date of the analysis\n",
    "# for regular \n",
    "ed_date = [dt.date.today().year, dt.date.today().month, dt.date.today().day]\n",
    "# end date will be \n",
    "st_datetime = dt.datetime(ed_date[0], ed_date[1], ed_date[2], 6, 0, 0) - dt.timedelta(days=360)\n",
    "st_date = [st_datetime.year, st_datetime.month, st_datetime.day]\n",
    "\n",
    "raw_data, raw_columns, clean_df, stationlist, raw_columns = readraw_data(destination, filename, foutname, st_date, ed_date)\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "This is the main part of the program\n",
    "below section will estimate the field capacity for the given data [Daily basis]\n",
    "\n",
    "This part should be done per station [stationlist]\n",
    "'''\n",
    "# stationlist = ['0111','0112','0114']\n",
    "print ('Station List ::',stationlist)\n",
    "\n",
    "# this is the target layer for the analysis. 1=layer 1\n",
    "# variable set for rainfinder\n",
    "# *-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= CHECK THE PARAMETERS =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-* #\n",
    "target = 'Layer2'\n",
    "threshold_moist = 15    # threshold of soil moisture difference to decide it was huge rain event or not default: 15\n",
    "raintimestep = 12 # 1 = 1/2 hour // 12 = 6 hours\n",
    "\n",
    "# variable set for FCfinder\n",
    "search_days = 14 # days\n",
    "search_range = 5 # hours\n",
    "search_slope = 0.07 # unit is fraction for standard deviation of the soil moisture behavior\n",
    "threshold_hour = 4 # hours with consistent soil moisture behavior\n",
    "# *-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= CHECK THE PARAMETERS =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-* #\n",
    "\n",
    "\n",
    "f=open(os.getcwd()+'/fc_results/'+strtoday+'_fc_record.csv', 'w')   # making a file to save the daily field capacity\n",
    "f.write('Station,Date,Layer1,Layer2,Layer3,Layer4\\n')\n",
    "f.close()\n",
    "\n",
    "for station in stationlist:\n",
    "    print('\\n\\n')\n",
    "    text = station +' :: '+target\n",
    "    print (text.center(60,'='))\n",
    "    bumplist = rainfinder(clean_df, station, raw_columns, threshold_moist, raintimestep)\n",
    "    for start_date in bumplist:\n",
    "        FCfinder(clean_df, station, raw_columns, start_date, search_days, search_range, search_slope, threshold_hour)\n",
    "\n",
    "\n",
    "update_fc() # update the field capacity data\n",
    "print ('Field Capacity Update is Done.\\n\\n\\n')\n",
    "\n",
    "\n",
    "'''\n",
    "this part will calculate the soil defecit based on the field capacity data exported from update_fc function\n",
    "'''\n",
    "rootdpth = 24\n",
    "ts = dt.date.today()\n",
    "ts = dt.date(2023, 7, 12)\n",
    "ts = pd.to_datetime(ts)\n",
    "deficit_calc(ts, rootdpth)\n",
    "\n",
    "print ('*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*')\n",
    "print ('Deficit Calculation is Done.\\n')\n",
    "print (\"Check the ./fc_results/\"+strtoday+\"_fc_record.csv folder for the today's field capacity data.\")\n",
    "print (\"Check the ./fc_graphs/ folder for the today's field capacity graph\")\n",
    "print (\"Check the ./00_Current_FieldCapacity.csv file for the most updated field capacity data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
